{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiogoLepri/ASD2_Project/blob/main/ASD-DiagNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mVAfnd06ES9",
        "outputId": "3b5fa4fa-47dd-4085-f621-c60f24d1b845"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ASD2_Project/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnJkylVA6DuC",
        "outputId": "2a52a746-45e6-4204-a252-9c7b775e37c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ASD2_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy matplotlib scikit-learn torch pyprind scipy\n"
      ],
      "metadata": {
        "id": "n0I3QvBKj_lM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4426d3e-583a-4167-b68d-0d5624e9c8cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "IF69HS76je7I"
      },
      "outputs": [],
      "source": [
        "#options: cc200, dosenbach160, aal\n",
        "p_ROI = \"cc200\"\n",
        "p_fold = 10\n",
        "p_center = \"Stanford\"\n",
        "p_mode = \"whole\"\n",
        "p_augmentation = True\n",
        "p_Method = \"ASD-DiagNet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1Oc4GXGje7K",
        "outputId": "f3646bb3-bb8e-4cec-ec9e-20f04061b56b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****List of patameters****\n",
            "ROI atlas:  cc200\n",
            "per Center or whole:  whole\n",
            "Method's name:  ASD-DiagNet\n",
            "Augmentation:  True\n"
          ]
        }
      ],
      "source": [
        "parameter_list = [p_ROI,p_fold,p_center,p_mode,p_augmentation,p_Method]\n",
        "print(\"*****List of patameters****\")\n",
        "print(\"ROI atlas: \",p_ROI)\n",
        "print(\"per Center or whole: \",p_mode)\n",
        "if p_mode == 'percenter':\n",
        "    print(\"Center's name: \",p_center)\n",
        "print(\"Method's name: \",p_Method)\n",
        "if p_Method == \"ASD-DiagNet\":\n",
        "    print(\"Augmentation: \",p_augmentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jUGCxT3qje7L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from functools import reduce\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import pyprind\n",
        "import sys\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy import stats\n",
        "from sklearn import tree\n",
        "import functools\n",
        "import numpy.ma as ma # for masked arrays\n",
        "import pyprind\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
        "from sklearn.impute import IterativeImputer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7-Rr-nGje7L"
      },
      "source": [
        "## Importing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bp8xZtF5je7M"
      },
      "outputs": [],
      "source": [
        "def get_key(filename):\n",
        "    f_split = filename.split('_')\n",
        "    if f_split[3] == 'rois':\n",
        "        key = '_'.join(f_split[0:3])\n",
        "    else:\n",
        "        key = '_'.join(f_split[0:2])\n",
        "    return key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_labels = pd.read_csv('/content/drive/MyDrive/ASD2_Project/Phenotypic_V1_0b_preprocessed1.csv')\n",
        "iq_columns = ['FIQ', 'PIQ', 'VIQ']\n",
        "\n",
        "\n",
        "for col in iq_columns:\n",
        "    df_labels[col] = pd.to_numeric(df_labels[col], errors='coerce')\n",
        "\n",
        "\n",
        "df_iq = df_labels[iq_columns].dropna()\n",
        "\n",
        "\n",
        "corr_matrix = df_iq.corr()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "plt.title(\"Correlation Matrix of IQ Measures\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "7ktK0syw2cAA",
        "outputId": "89291ec4-54ad-4d0f-88e5-9f6b2f8608b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Matrix:\n",
            "          FIQ       PIQ       VIQ\n",
            "FIQ  1.000000  0.837772  0.756112\n",
            "PIQ  0.837772  1.000000  0.902055\n",
            "VIQ  0.756112  0.902055  1.000000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_main_path = f'/content/drive/MyDrive/ASD2_Project/Outputs/cpac/filt_global/rois_{p_ROI}'\n",
        "flist = os.listdir(data_main_path)\n",
        "print(len(flist))\n",
        "\n",
        "for f in range(len(flist)):\n",
        "    flist[f] = get_key(flist[f])\n",
        "\n",
        "\n",
        "df_labels = pd.read_csv('/content/drive/MyDrive/ASD2_Project/Phenotypic_V1_0b_preprocessed1.csv')\n",
        "df_labels.DX_GROUP = df_labels.DX_GROUP.map({1: 1, 2: 0})\n",
        "print(len(df_labels))\n",
        "\n",
        "labels = {}\n",
        "for i, row in df_labels.iterrows():\n",
        "    file_id = row['FILE_ID']\n",
        "    y_label = row['DX_GROUP']\n",
        "    if file_id == 'no_filename':\n",
        "        continue\n",
        "    assert(file_id not in labels)\n",
        "    labels[file_id] = y_label\n",
        "\n",
        "#new age\n",
        "df_labels['AGE_AT_SCAN'] = df_labels['AGE_AT_SCAN'].astype(np.float32)\n",
        "ages = df_labels['AGE_AT_SCAN']\n",
        "ages_norm = ((ages - ages.mean()) / ages.std()).round(1)\n",
        "\n",
        "#IQS\n",
        "iq_columns = ['FIQ', 'PIQ', 'VIQ']\n",
        "\n",
        "\n",
        "for col in iq_columns:\n",
        "    df_labels.loc[df_labels[col] == -9999, col] = np.nan\n",
        "\n",
        "\n",
        "imputer = IterativeImputer(random_state=0)\n",
        "df_labels[iq_columns] = imputer.fit_transform(df_labels[iq_columns])\n",
        "\n",
        "\n",
        "phen_dict = {}\n",
        "for i, row in df_labels.iterrows():\n",
        "    file_id = row['FILE_ID']\n",
        "    if file_id == 'no_filename':\n",
        "        continue\n",
        "    phen_dict[file_id] = [ages_norm.iloc[i], row['FIQ'], row['VIQ'], row['PIQ']]\n",
        "#phen_dict[file_id] = [ages_norm.iloc[i], row['FIQ'], row['VIQ'], row['PIQ']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40VjQNF33Yie",
        "outputId": "64e01201-4c3b-47a9-e1fa-a72e484b2cc5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "884\n",
            "1112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/impute/_iterative.py:895: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(phen_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tF9Z8z4FJ4_S",
        "outputId": "e9a7b384-8ad3-495e-dc9f-aa475320de62"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Pitt_0050003': [0.9, 124.0, 128.0, 115.0], 'Pitt_0050004': [0.3, 113.0, 108.0, 117.0], 'Pitt_0050005': [-0.4, 119.0, 117.0, 118.0], 'Pitt_0050006': [-0.5, 109.0, 99.0, 119.0], 'Pitt_0050007': [0.1, 110.0, 106.0, 112.0], 'Pitt_0050008': [1.9, 123.0, 123.0, 114.0], 'Pitt_0050009': [2.1, 126.0, 118.0, 128.0], 'Pitt_0050010': [2.3, 81.0, 81.0, 93.0], 'Pitt_0050011': [-0.0, 111.0, 101.0, 120.0], 'Pitt_0050012': [0.6, 128.0, 119.0, 128.0], 'Pitt_0050013': [-1.0, 86.0, 89.0, 87.0], 'Pitt_0050014': [-0.4, 96.0, 97.0, 96.0], 'Pitt_0050015': [-0.4, 99.0, 98.0, 99.0], 'Pitt_0050016': [0.6, 123.0, 120.0, 119.0], 'Pitt_0050017': [0.7, 87.0, 91.0, 86.0], 'Pitt_0050019': [1.3, 100.0, 89.0, 111.0], 'Pitt_0050020': [0.5, 100.0, 120.0, 83.0], 'Pitt_0050022': [-0.0, 119.0, 110.0, 110.0], 'Pitt_0050023': [-0.5, 97.0, 83.0, 111.0], 'Pitt_0050024': [0.7, 127.0, 121.0, 126.0], 'Pitt_0050025': [1.8, 131.0, 132.0, 123.0], 'Pitt_0050026': [-0.1, 87.0, 90.0, 88.0], 'Pitt_0050027': [-0.6, 98.0, 98.0, 98.0], 'Pitt_0050028': [-0.5, 126.0, 112.0, 128.0], 'Pitt_0050029': [-0.7, 106.0, 109.0, 100.0], 'Pitt_0050030': [1.0, 105.0, 100.0, 108.0], 'Pitt_0050031': [-0.5, 106.0, 102.0, 107.0], 'Pitt_0050032': [0.3, 119.0, 116.0, 119.0], 'Pitt_0050033': [-0.6, 98.0, 98.0, 96.0], 'Pitt_0050034': [-0.3, 97.0, 98.0, 96.0], 'Pitt_0050035': [0.0, 107.0, 109.0, 102.0], 'Pitt_0050036': [-0.5, 103.0, 94.0, 112.0], 'Pitt_0050037': [0.3, 115.0, 112.0, 115.0], 'Pitt_0050038': [-0.4, 130.0, 132.0, 121.0], 'Pitt_0050039': [1.7, 118.0, 110.0, 123.0], 'Pitt_0050040': [0.9, 110.0, 109.0, 108.0], 'Pitt_0050041': [1.4, 103.0, 108.0, 96.0], 'Pitt_0050042': [2.0, 109.0, 111.0, 104.0], 'Pitt_0050043': [-0.4, 113.0, 115.0, 107.0], 'Pitt_0050044': [0.0, 110.0, 117.0, 102.0], 'Pitt_0050045': [-0.2, 114.0, 115.0, 109.0], 'Pitt_0050046': [0.4, 100.0, 88.0, 111.0], 'Pitt_0050047': [-0.2, 103.0, 95.0, 109.0], 'Pitt_0050048': [-0.7, 107.0, 98.0, 117.0], 'Pitt_0050049': [-0.2, 100.0, 96.0, 103.0], 'Pitt_0050050': [-0.3, 113.0, 104.0, 121.0], 'Pitt_0050051': [-0.5, 107.0, 101.0, 110.0], 'Pitt_0050052': [2.0, 122.0, 123.0, 115.0], 'Pitt_0050053': [-0.6, 122.0, 119.0, 119.0], 'Pitt_0050054': [-0.9, 95.0, 99.0, 90.0], 'Pitt_0050055': [-0.0, 111.0, 108.0, 112.0], 'Pitt_0050056': [-0.4, 113.0, 101.0, 125.0], 'Pitt_0050057': [-0.6, 124.0, 115.0, 128.0], 'Pitt_0050058': [0.8, 127.0, 128.0, 119.0], 'Pitt_0050059': [0.5, 117.0, 109.0, 121.0], 'Pitt_0050060': [0.4, 126.0, 122.0, 119.0], 'Olin_0050102': [-0.4, 103.0, 103.71138940227638, 101.757795052487], 'Olin_0050103': [-0.4, 106.0, 106.18717094753204, 104.66297048244367], 'Olin_0050104': [-0.1, 125.0, 121.86712073415124, 123.06241487216919], 'Olin_0050105': [-0.0, 100.0, 101.23560785702071, 98.85261962253036], 'Olin_0050106': [-0.9, 129.0, 125.16816279449213, 126.9359821121114], 'Olin_0050107': [0.5, 132.0, 127.64394433974782, 129.84115754206803], 'Olin_0050109': [0.4, 102.0, 102.88612888719118, 100.78940324250144], 'Olin_0050110': [-0.5, 79.0, 83.9051370402311, 78.5163916128337], 'Olin_0050111': [-0.4, 118.0, 116.09029712855468, 116.28367220227032], 'Olin_0050112': [-0.0, 102.0, 102.88612888719118, 100.78940324250144], 'Olin_0050113': [-0.3, 100.0, 101.23560785702071, 98.85261962253036], 'Olin_0050114': [0.4, 127.0, 123.5176417643217, 124.99919849214028], 'Olin_0050115': [0.7, 135.0, 130.11972588500345, 132.74633297202473], 'Olin_0050116': [-0.3, 115.0, 113.61451558329904, 113.37849677231364], 'Olin_0050117': [0.5, 135.0, 130.11972588500345, 132.74633297202473], 'Olin_0050118': [-0.3, 80.0, 84.73039755531632, 79.48478342281926], 'Olin_0050119': [0.2, 132.0, 127.64394433974782, 129.84115754206803], 'Olin_0050120': [-0.3, 103.0, 103.71138940227638, 101.757795052487], 'Olin_0050121': [0.4, 118.0, 116.09029712855468, 116.28367220227032], 'Olin_0050122': [-0.6, 112.0, 111.13873403804335, 110.473321342357], 'Olin_0050123': [-0.0, 135.0, 130.11972588500345, 132.74633297202473], 'Olin_0050124': [0.1, 100.0, 101.23560785702071, 98.85261962253036], 'Olin_0050125': [0.9, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'Olin_0050126': [-0.0, 133.0, 128.46920485483298, 130.80954935205364], 'Olin_0050127': [-0.4, 115.0, 113.61451558329904, 113.37849677231364], 'Olin_0050128': [-0.1, 114.0, 112.78925506821383, 112.41010496232808], 'Olin_0050129': [-0.6, 108.0, 107.83769197770249, 106.59975410241478], 'Olin_0050130': [0.1, 129.0, 125.16816279449213, 126.9359821121114], 'Olin_0050131': [0.5, 71.0, 77.30305291954929, 70.7692571329493], 'Olin_0050132': [-0.1, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'Olin_0050133': [-0.8, 112.0, 111.13873403804335, 110.473321342357], 'Olin_0050134': [0.1, 124.0, 121.04186021906602, 122.09402306218362], 'Olin_0050135': [-0.6, 97.0, 98.75982631176504, 95.9474441925737], 'Olin_0050136': [0.2, 131.0, 126.81868382466259, 128.87276573208248], 'OHSU_0050142': [-0.4, 124.0, 121.04186021906602, 122.09402306218362], 'OHSU_0050143': [-0.4, 130.0, 125.99342330957734, 127.90437392209695], 'OHSU_0050144': [-0.8, 110.0, 109.48821300787293, 108.53653772238589], 'OHSU_0050145': [-0.8, 82.0, 86.38091858548675, 81.42156704279037], 'OHSU_0050146': [-1.1, 132.0, 127.64394433974782, 129.84115754206803], 'OHSU_0050147': [-0.7, 128.0, 124.34290227940693, 125.96759030212583], 'OHSU_0050148': [-0.5, 69.6, 76.14768819842999, 69.41350859896951], 'OHSU_0050149': [-0.6, 107.6, 107.50758777166838, 106.21239737842056], 'OHSU_0050150': [-0.9, 77.2, 82.41966811307766, 76.77328635485975], 'OHSU_0050152': [-0.9, 105.7, 105.93959279300648, 104.37245293944801], 'OHSU_0050153': [-0.9, 100.0, 101.23560785702071, 98.85261962253036], 'OHSU_0050156': [-0.2, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'OHSU_0050157': [-0.8, 104.0, 104.53664991736161, 102.72618686247255], 'OHSU_0050158': [-0.7, 106.0, 106.18717094753204, 104.66297048244367], 'OHSU_0050159': [-0.8, 132.0, 127.64394433974782, 129.84115754206803], 'OHSU_0050160': [-0.7, 112.0, 111.13873403804335, 110.473321342357], 'OHSU_0050161': [-1.0, 104.0, 104.53664991736161, 102.72618686247255], 'OHSU_0050162': [-1.0, 120.0, 117.74081815872515, 118.22045582224142], 'OHSU_0050163': [-1.0, 118.7, 116.66797948911437, 116.96154646926017], 'OHSU_0050164': [-1.0, 122.0, 119.39133918889556, 120.15723944221254], 'OHSU_0050166': [-1.1, 126.0, 122.69238124923645, 124.03080668215476], 'OHSU_0050167': [-0.9, 115.2, 113.77956768631611, 113.57217513431074], 'OHSU_0050168': [-0.9, 105.7, 105.93959279300648, 104.37245293944801], 'OHSU_0050169': [-0.6, 130.0, 125.99342330957734, 127.90437392209695], 'OHSU_0050170': [-0.8, 94.0, 96.28404476650941, 93.042268762617], 'OHSU_0050171': [-0.8, 120.0, 117.74081815872515, 118.22045582224142], 'SDSU_0050182': [-0.1, 115.0, 113.0, 114.0], 'SDSU_0050183': [-0.4, 139.0, 128.0, 140.0], 'SDSU_0050184': [0.0, 141.0, 147.0, 126.0], 'SDSU_0050185': [-0.6, 94.0, 88.0, 101.0], 'SDSU_0050186': [-0.6, 112.0, 109.0, 110.0], 'SDSU_0050187': [-0.1, 113.0, 119.0, 104.0], 'SDSU_0050188': [-0.2, 126.0, 116.0, 129.0], 'SDSU_0050189': [-0.1, 115.0, 109.0, 117.0], 'SDSU_0050190': [-0.4, 124.0, 125.0, 118.0], 'SDSU_0050191': [-0.5, 114.0, 125.0, 101.0], 'SDSU_0050192': [-0.5, 85.0, 93.0, 81.0], 'SDSU_0050193': [-0.3, 112.0, 108.0, 114.0], 'SDSU_0050194': [-0.5, 110.0, 104.0, 116.0], 'SDSU_0050195': [-0.6, 123.0, 111.0, 129.0], 'SDSU_0050196': [-0.5, 110.0, 116.0, 103.0], 'SDSU_0050197': [-0.3, 109.0, 109.0, 106.0], 'SDSU_0050198': [-0.2, 121.0, 117.0, 120.0], 'SDSU_0050199': [-0.2, 100.0, 107.0, 92.0], 'SDSU_0050200': [-0.4, 108.0, 107.0, 106.0], 'SDSU_0050201': [-0.4, 98.0, 106.0, 90.0], 'SDSU_0050202': [-0.6, 100.0, 87.0, 114.0], 'SDSU_0050203': [-0.1, 117.0, 117.0, 114.0], 'SDSU_0050204': [-0.3, 100.0, 89.0, 110.0], 'SDSU_0050205': [-0.4, 109.0, 105.0, 112.0], 'SDSU_0050206': [-1.0, 112.0, 119.0, 103.0], 'SDSU_0050207': [-0.1, 81.0, 83.0, 84.0], 'SDSU_0050208': [-0.4, 93.0, 103.0, 86.0], 'SDSU_0050209': [-0.1, 126.0, 126.0, 120.0], 'SDSU_0050210': [-0.1, 123.0, 114.0, 126.0], 'SDSU_0050211': [-0.1, 105.0, 112.0, 96.0], 'SDSU_0050212': [-0.4, 100.0, 88.0, 112.0], 'SDSU_0050213': [-0.6, 88.0, 93.0, 88.0], 'SDSU_0050214': [-0.3, 93.0, 87.0, 101.0], 'SDSU_0050215': [-0.0, 117.0, 110.0, 120.0], 'SDSU_0050216': [-0.2, 100.0, 99.0, 99.0], 'SDSU_0050217': [-0.5, 104.0, 100.0, 106.0], 'Trinity_0050232': [0.8, 115.0, 123.0, 103.0], 'Trinity_0050233': [0.5, 135.0, 131.0, 131.0], 'Trinity_0050234': [-0.2, 111.0, 104.0, 117.0], 'Trinity_0050235': [-0.2, 119.0, 122.0, 111.0], 'Trinity_0050236': [0.3, 118.0, 109.0, 123.0], 'Trinity_0050237': [0.5, 118.0, 109.0, 116.0], 'Trinity_0050239': [-0.0, 97.0, 92.0, 102.0], 'Trinity_0050240': [-0.2, 132.0, 135.0, 121.0], 'Trinity_0050241': [0.0, 89.0, 95.0, 87.0], 'Trinity_0050242': [-0.5, 117.0, 116.0, 117.0], 'Trinity_0050243': [-0.4, 90.0, 98.0, 88.0], 'Trinity_0050245': [0.6, 72.0, 85.0, 63.0], 'Trinity_0050246': [0.3, 105.0, 108.0, 99.0], 'Trinity_0050247': [-0.6, 103.0, 99.0, 105.93786809906841], 'Trinity_0050248': [-0.4, 117.0, 112.0, 117.0], 'Trinity_0050249': [-0.4, 98.0, 98.0, 99.0], 'Trinity_0050250': [-0.4, 99.0, 99.0, 98.0], 'Trinity_0050251': [0.0, 122.0, 119.0, 119.0], 'Trinity_0050252': [-0.2, 94.0, 87.0, 102.0], 'Trinity_0050253': [0.1, 99.0, 90.0, 106.0], 'Trinity_0050254': [0.3, 121.0, 123.0, 114.0], 'Trinity_0050255': [-0.3, 125.0, 119.0, 126.0], 'Trinity_0050257': [-0.3, 113.0, 105.0, 120.0], 'Trinity_0050259': [0.3, 114.0, 118.0, 107.0], 'Trinity_0050260': [0.4, 116.0, 119.0, 116.0], 'Trinity_0050261': [0.4, 116.0, 113.0, 116.0], 'Trinity_0050262': [0.5, 133.0, 127.0, 132.0], 'Trinity_0050263': [0.3, 117.0, 119.0, 110.0], 'Trinity_0050264': [1.1, 120.0, 115.0, 120.0], 'Trinity_0050265': [-0.6, 101.0, 97.0, 105.0], 'Trinity_0050266': [-0.1, 116.0, 118.0, 109.0], 'Trinity_0050267': [0.1, 116.0, 114.0, 115.0], 'Trinity_0050268': [-0.3, 96.0, 107.0, 86.0], 'Trinity_0050269': [-0.3, 113.0, 110.0, 112.0], 'Trinity_0050270': [0.3, 99.0, 98.0, 99.0], 'Trinity_0050271': [0.3, 105.0, 96.0, 114.0], 'UM_1_0050272': [-0.4, 98.5, 100.0, 97.0], 'UM_1_0050273': [-0.0, 112.5, 109.0, 116.0], 'UM_1_0050274': [-0.4, 111.5, 106.0, 117.0], 'UM_1_0050275': [-0.7, 85.0, 106.0, 64.0], 'UM_1_0050276': [-0.0, 146.5, 145.0, 148.0], 'UM_1_0050277': [-0.2, 107.5, 96.0, 119.0], 'UM_1_0050278': [-0.7, 107.5, 93.0, 122.0], 'UM_1_0050279': [-0.6, 87.5, 75.0, 100.0], 'UM_1_0050280': [-0.2, 85.0, 81.0, 89.0], 'UM_1_0050281': [-0.8, 132.0, 131.0, 133.0], 'UM_1_0050282': [-0.2, 121.0, 118.0, 124.0], 'UM_1_0050283': [-0.7, 97.5, 91.0, 104.0], 'UM_1_0050284': [-0.7, 111.0, 113.0, 109.0], 'UM_1_0050285': [-0.5, 99.5, 94.0, 105.0], 'UM_1_0050286': [-0.7, 130.5, 139.0, 122.0], 'UM_1_0050287': [-0.3, 96.5, 90.0, 103.0], 'UM_1_0050288': [-0.7, 105.0, 104.0, 106.0], 'UM_1_0050289': [-0.5, 105.0, 100.0, 110.0], 'UM_1_0050290': [-0.4, 108.5, 103.0, 114.0], 'UM_1_0050291': [-0.4, 96.0, 98.0, 94.0], 'UM_1_0050292': [-0.6, 89.5, 91.0, 88.0], 'UM_1_0050293': [-0.6, 120.34876906711513, 126.0, 111.40842076818186], 'UM_1_0050294': [-0.6, 103.0, 110.0, 96.0], 'UM_1_0050295': [-0.7, 135.0, 133.0, 137.0], 'UM_1_0050296': [-0.2, 89.0, 78.0, 100.0], 'UM_1_0050297': [-0.1, 125.5, 141.0, 110.0], 'UM_1_0050298': [-0.5, 101.0, 121.0, 81.0], 'UM_1_0050299': [-0.6, 87.5, 96.0, 79.0], 'UM_1_0050300': [-0.9, 90.0, 99.0, 81.0], 'UM_1_0050301': [-0.1, 97.0, 101.0, 93.0], 'UM_1_0050302': [-0.8, 97.0, 109.0, 85.0], 'UM_1_0050303': [-0.7, 84.0, 75.0, 93.0], 'UM_1_0050304': [-0.8, 96.5, 99.0, 94.0], 'UM_1_0050305': [-0.8, 85.0, 103.0, 67.0], 'UM_1_0050306': [-1.1, 76.0, 86.0, 66.0], 'UM_1_0050307': [-0.9, 81.5, 87.0, 76.0], 'UM_1_0050308': [-1.0, 113.0, 113.0, 113.0], 'UM_1_0050310': [-0.9, 78.5, 82.0, 75.0], 'UM_1_0050311': [-0.9, 89.0, 96.0, 82.0], 'UM_1_0050312': [-0.7, 91.0, 97.0, 85.0], 'UM_1_0050313': [-0.5, 101.0, 109.0, 93.0], 'UM_1_0050314': [-0.1, 126.0, 127.0, 125.0], 'UM_1_0050315': [-0.5, 118.5, 126.0, 111.0], 'UM_1_0050316': [-0.7, 109.0, 125.0, 93.0], 'UM_1_0050317': [-0.9, 115.0, 117.0, 113.0], 'UM_1_0050318': [-1.0, 147.5, 180.0, 115.0], 'UM_1_0050319': [-0.3, 126.0, 125.0, 127.0], 'UM_1_0050320': [0.2, 108.0, 89.0, 127.0], 'UM_1_0050321': [0.1, 98.5, 116.0, 81.0], 'UM_1_0050322': [-0.6, 92.5, 103.0, 82.0], 'UM_1_0050324': [-0.4, 124.5, 136.0, 113.0], 'UM_1_0050325': [-0.5, 77.0, 95.0, 59.0], 'UM_1_0050326': [-0.6, 79.0, 88.0, 70.0], 'UM_1_0050327': [-0.2, 111.0, 107.0, 115.0], 'UM_1_0050329': [0.0, 100.0, 107.0, 93.0], 'UM_1_0050330': [-0.1, 120.0, 147.0, 93.0], 'UM_1_0050331': [0.0, 128.4016460971834, 138.0, 114.40849722624426], 'UM_1_0050332': [-0.8, 103.5, 121.0, 86.0], 'UM_1_0050333': [-0.9, 100.5, 101.0, 100.0], 'UM_1_0050334': [-0.8, 108.0, 108.0, 108.0], 'UM_1_0050335': [0.1, 108.0, 120.0, 96.0], 'UM_1_0050336': [-0.3, 108.0, 113.0, 103.0], 'UM_1_0050337': [-0.7, 112.0, 126.0, 98.0], 'UM_1_0050338': [-0.4, 106.5, 111.0, 102.0], 'UM_1_0050339': [0.1, 115.65125746624204, 119.0, 109.65837616764559], 'UM_1_0050340': [-0.1, 110.0, 110.0, 110.0], 'UM_1_0050341': [0.1, 118.33554980959808, 123.0, 110.65840165366629], 'UM_1_0050342': [-0.6, 103.5, 107.0, 100.0], 'UM_1_0050343': [-0.4, 124.5, 136.0, 113.0], 'UM_1_0050344': [0.1, 121.5, 126.0, 117.0], 'UM_1_0050345': [0.0, 118.5, 122.0, 115.0], 'UM_1_0050346': [0.2, 115.5, 120.0, 111.0], 'UM_1_0050347': [-0.2, 122.0, 129.0, 115.0], 'UM_1_0050348': [0.3, 112.5, 114.0, 111.0], 'UM_1_0050349': [0.2, 111.0, 116.0, 106.0], 'UM_1_0050350': [-0.6, 109.5, 109.0, 110.0], 'UM_1_0050351': [-0.0, 116.0, 107.0, 125.0], 'UM_1_0050352': [-0.2, 113.5, 121.0, 106.0], 'UM_1_0050353': [-0.5, 89.0, 96.0, 82.0], 'UM_1_0050354': [-0.4, 103.0, 110.0, 96.0], 'UM_1_0050355': [-0.8, 127.5, 138.0, 117.0], 'UM_1_0050356': [0.0, 92.5, 86.0, 99.0], 'UM_1_0050357': [-0.9, 92.5, 103.0, 82.0], 'UM_1_0050358': [-0.9, 99.5, 106.0, 93.0], 'UM_1_0050359': [-0.8, 85.0, 95.0, 75.0], 'UM_1_0050360': [-0.4, 113.5, 119.0, 108.0], 'UM_1_0050361': [0.2, 106.5, 100.0, 113.0], 'UM_1_0050362': [-0.7, 108.0, 106.0, 110.0], 'UM_1_0050363': [-0.9, 104.5, 103.0, 106.0], 'UM_1_0050364': [-0.8, 97.0, 102.0, 92.0], 'UM_1_0050365': [-0.2, 106.5, 98.0, 115.0], 'UM_1_0050366': [-1.1, 102.0, 114.0, 90.0], 'UM_1_0050367': [-0.8, 105.0, 112.0, 98.0], 'UM_1_0050368': [0.1, 118.5, 124.0, 113.0], 'UM_1_0050369': [-0.3, 96.5, 97.0, 96.0], 'UM_1_0050370': [-0.5, 93.5, 98.0, 89.0], 'UM_1_0050371': [-0.6, 97.5, 123.0, 72.0], 'UM_1_0050372': [-0.9, 107.5, 109.0, 106.0], 'UM_1_0050373': [0.0, 94.0, 94.0, 94.0], 'UM_1_0050374': [-0.9, 106.0, 120.0, 92.0], 'UM_1_0050375': [-0.7, 123.5, 137.0, 110.0], 'UM_1_0050376': [-1.0, 98.5, 118.0, 79.0], 'UM_1_0050377': [-0.7, 113.5, 129.0, 98.0], 'UM_1_0050379': [-0.3, 104.0, 116.0, 92.0], 'UM_1_0050380': [0.0, 95.5, 88.0, 103.0], 'UM_1_0050381': [-0.1, 114.5, 121.0, 108.0], 'UM_2_0050382': [1.5, 114.0, 115.0, 109.0], 'UM_2_0050383': [-0.3, 116.0, 111.0, 117.0], 'UM_2_0050385': [0.1, 109.0, 99.0, 119.0], 'UM_2_0050386': [0.1, 129.0, 126.0, 119.0], 'UM_2_0050387': [-0.4, 93.5, 98.0, 89.0], 'UM_2_0050388': [1.2, 115.0, 108.0, 119.0], 'UM_2_0050390': [-0.3, 103.5, 107.0, 100.0], 'UM_2_0050391': [0.0, 113.5, 121.0, 106.0], 'UM_2_0050397': [-0.3, 116.0, 132.0, 100.0], 'UM_2_0050399': [0.0, 104.5, 129.0, 80.0], 'UM_2_0050402': [-0.5, 121.0, 117.0, 125.0], 'UM_2_0050403': [-0.5, 120.5, 126.0, 115.0], 'UM_2_0050404': [-0.2, 129.5, 132.0, 127.0], 'UM_2_0050405': [-0.1, 94.0, 90.0, 98.0], 'UM_2_0050406': [-0.2, 90.5, 78.0, 103.0], 'UM_2_0050407': [-0.1, 105.0, 107.0, 103.0], 'UM_2_0050408': [-0.3, 118.5, 123.0, 114.0], 'UM_2_0050410': [-0.1, 110.5, 113.0, 108.0], 'UM_2_0050411': [-0.4, 125.5, 126.0, 125.0], 'UM_2_0050412': [-0.5, 133.5, 137.0, 130.0], 'UM_2_0050413': [-0.5, 114.5, 96.0, 133.0], 'UM_2_0050414': [-0.4, 109.0, 132.0, 86.0], 'UM_2_0050415': [0.1, 111.5, 117.0, 106.0], 'UM_2_0050416': [-0.3, 108.5, 114.0, 103.0], 'UM_2_0050417': [-0.5, 121.5, 128.0, 115.0], 'UM_2_0050418': [-0.3, 112.0, 109.0, 115.0], 'UM_2_0050419': [-0.2, 111.0, 95.0, 127.0], 'UM_2_0050421': [-0.2, 116.0, 126.0, 106.0], 'UM_2_0050422': [-0.3, 114.5, 119.0, 110.0], 'UM_2_0050424': [0.1, 125.5, 136.0, 115.0], 'UM_2_0050425': [-0.1, 112.0, 109.0, 115.0], 'UM_2_0050426': [-0.4, 113.0, 115.0, 111.0], 'UM_2_0050427': [-0.2, 96.0, 100.0, 92.0], 'UM_2_0050428': [-0.2, 89.5, 98.0, 81.0], 'USM_0050433': [0.2, 100.0, 98.0, 103.0], 'USM_0050434': [0.2, 93.0, 88.0, 99.0], 'USM_0050435': [-0.1, 100.0, 98.0, 102.0], 'USM_0050436': [-0.4, 126.0, 120.0, 125.0], 'USM_0050437': [-0.3, 138.0, 139.0, 127.0], 'USM_0050438': [-0.3, 97.0, 87.0, 108.0], 'USM_0050439': [0.7, 128.0, 130.0, 121.0], 'USM_0050440': [0.9, 98.0, 104.0, 90.0], 'USM_0050441': [1.3, 123.0, 118.0, 121.0], 'USM_0050442': [1.2, 127.0, 128.0, 119.0], 'USM_0050443': [0.0, 120.0, 99.0, 122.0], 'USM_0050444': [1.0, 100.0, 106.0, 92.0], 'USM_0050445': [0.1, 128.0, 126.0, 125.0], 'USM_0050446': [1.3, 116.0, 118.0, 111.0], 'USM_0050447': [-0.5, 130.0, 132.0, 121.0], 'USM_0050448': [-0.8, 144.0, 140.0, 138.0], 'USM_0050449': [0.1, 108.0, 104.0, 111.0], 'USM_0050453': [-1.0, 121.0, 104.0, 137.0], 'USM_0050455': [1.8, 112.0, 111.0, 109.0], 'USM_0050463': [1.4, 121.0, 124.0, 113.0], 'USM_0050466': [2.8, 103.0, 101.0, 106.0], 'USM_0050467': [0.3, 89.0, 89.0, 90.0], 'USM_0050468': [2.8, 122.0, 127.0, 111.0], 'USM_0050469': [1.4, 134.0, 130.0, 130.0], 'USM_0050470': [-0.9, 106.0, 119.0, 90.0], 'USM_0050477': [0.4, 92.0, 95.0, 91.0], 'USM_0050480': [1.5, 127.0, 117.0, 132.0], 'USM_0050481': [0.0, 88.0, 85.0, 95.0], 'USM_0050482': [1.3, 94.0, 94.0, 92.0], 'USM_0050483': [0.5, 96.0, 76.0, 120.0], 'USM_0050485': [0.8, 132.0, 122.0, 133.0], 'USM_0050486': [0.1, 97.0, 77.0, 120.0], 'USM_0050487': [1.1, 86.0, 77.0, 99.0], 'USM_0050488': [1.0, 100.0, 83.0, 120.0], 'USM_0050489': [-0.0, 94.0, 72.0, 124.0], 'USM_0050490': [0.3, 110.0, 100.0, 119.0], 'USM_0050491': [1.2, 125.0, 122.0, 120.0], 'USM_0050492': [1.9, 126.0, 130.0, 117.0], 'USM_0050493': [2.3, 102.0, 98.0, 106.0], 'USM_0050494': [2.6, 65.0, 55.0, 76.0], 'USM_0050496': [0.9, 129.0, 120.0, 132.0], 'USM_0050497': [2.2, 80.0, 76.0, 89.0], 'USM_0050498': [0.5, 99.0, 118.0, 87.0], 'USM_0050499': [1.0, 113.0, 121.0, 104.0], 'USM_0050500': [0.0, 118.0, 113.0, 119.0], 'USM_0050501': [0.1, 78.0, 79.0, 80.0], 'USM_0050502': [2.0, 90.0, 101.0, 83.0], 'USM_0050503': [1.4, 80.0, 91.0, 72.0], 'USM_0050504': [0.1, 87.0, 86.0, 92.0], 'USM_0050505': [2.0, 95.0, 86.0, 106.0], 'USM_0050507': [1.4, 97.0, 108.0, 87.0], 'USM_0050509': [-0.0, 95.0, 85.0, 107.0], 'USM_0050510': [-0.4, 102.0, 104.0, 99.0], 'USM_0050511': [-0.1, 114.0, 109.0, 116.0], 'USM_0050514': [0.5, 109.0, 99.0, 119.0], 'USM_0050515': [-0.1, 94.0, 71.0, 106.0], 'USM_0050516': [0.0, 107.0, 108.0, 104.0], 'USM_0050518': [0.2, 77.0, 83.0, 75.0], 'USM_0050519': [-0.1, 83.0, 78.0, 90.0], 'USM_0050520': [0.1, 76.0, 55.0, 101.0], 'USM_0050521': [0.2, 80.0, 69.0, 99.0], 'USM_0050523': [0.0, 115.0, 118.0, 109.0], 'USM_0050524': [-0.6, 83.0, 76.0, 93.0], 'USM_0050525': [2.0, 106.0, 87.0, 128.0], 'USM_0050526': [4.1, 123.0, 118.0, 121.0], 'USM_0050527': [0.2, 93.0, 84.0, 105.0], 'USM_0050528': [-0.7, 100.0, 90.0, 109.0], 'USM_0050529': [3.1, 128.0, 122.0, 127.0], 'USM_0050530': [0.1, 106.0, 120.0, 91.0], 'USM_0050531': [1.4, 112.0, 104.0, 119.0], 'USM_0050532': [-0.0, 84.0, 87.0, 81.0], 'Yale_0050551': [-0.1, 94.0, 107.0, 83.0], 'Yale_0050552': [-0.5, 106.0, 106.0, 95.0], 'Yale_0050553': [-0.9, 127.0, 119.0, 125.0], 'Yale_0050554': [-1.0, 99.0, 87.0, 95.0], 'Yale_0050555': [-0.3, 131.0, 122.0, 107.0], 'Yale_0050556': [-0.8, 78.0, 89.0, 76.0], 'Yale_0050557': [-0.3, 92.0, 92.0, 93.0], 'Yale_0050558': [-0.4, 113.0, 101.0, 115.0], 'Yale_0050559': [-0.6, 123.0, 116.0, 115.0], 'Yale_0050560': [-1.0, 111.0, 106.0, 108.0], 'Yale_0050561': [-0.5, 110.0, 124.0, 99.0], 'Yale_0050562': [-0.8, 114.0, 120.0, 107.0], 'Yale_0050563': [-0.4, 137.0, 140.0, 139.0], 'Yale_0050564': [-1.2, 140.0, 126.0, 134.0], 'Yale_0050565': [-0.8, 101.0, 113.0, 104.0], 'Yale_0050566': [-0.9, 107.0, 113.0, 106.0], 'Yale_0050567': [0.1, 80.0, 86.0, 79.0], 'Yale_0050568': [-0.4, 92.0, 107.0, 91.0], 'Yale_0050569': [-0.0, 89.0, 83.0, 93.0], 'Yale_0050570': [-0.0, 88.0, 103.0, 84.0], 'Yale_0050571': [-0.8, 92.0, 91.0, 97.0], 'Yale_0050572': [-0.2, 105.0, 126.0, 82.0], 'Yale_0050573': [-0.4, 103.0, 92.0, 103.0], 'Yale_0050574': [-0.8, 91.0, 103.0, 88.0], 'Yale_0050575': [-0.4, 120.0, 127.0, 114.0], 'Yale_0050576': [-1.1, 119.0, 112.0, 120.0], 'Yale_0050577': [-0.2, 73.0, 73.0, 77.0], 'Yale_0050578': [-0.6, 104.0, 106.0, 106.0], 'Yale_0050601': [-0.3, 102.0, 115.0, 104.0], 'Yale_0050602': [-0.9, 108.0, 106.0, 109.0], 'Yale_0050603': [-0.5, 76.0, 69.0, 88.0], 'Yale_0050604': [-0.3, 85.0, 79.0, 84.0], 'Yale_0050605': [-0.1, 121.0, 102.0, 111.0], 'Yale_0050606': [-0.1, 41.0, 42.0, 37.0], 'Yale_0050607': [0.0, 141.0, 143.0, 120.0], 'Yale_0050608': [-0.1, 100.0, 102.0, 99.0], 'Yale_0050609': [-1.0, 114.0, 117.0, 107.0], 'Yale_0050610': [-0.5, 95.0, 107.0, 84.0], 'Yale_0050611': [-0.6, 86.0, 100.0, 87.0], 'Yale_0050612': [-0.6, 121.0, 126.0, 107.0], 'Yale_0050613': [-0.6, 131.0, 141.0, 112.0], 'Yale_0050614': [0.1, 72.0, 57.0, 67.0], 'Yale_0050615': [-1.1, 119.0, 101.0, 126.0], 'Yale_0050616': [-0.6, 93.0, 106.0, 95.0], 'Yale_0050617': [-1.2, 84.0, 82.0, 76.0], 'Yale_0050618': [-0.5, 76.0, 79.0, 93.0], 'Yale_0050619': [-0.1, 88.0, 86.0, 94.0], 'Yale_0050620': [-0.5, 114.0, 110.0, 115.0], 'Yale_0050621': [-0.1, 83.0, 111.0, 77.0], 'Yale_0050622': [-0.9, 89.0, 97.0, 100.0], 'Yale_0050623': [-0.4, 94.0, 101.0, 86.0], 'Yale_0050624': [-0.7, 90.0, 91.0, 89.0], 'Yale_0050625': [-1.3, 99.0, 90.0, 98.0], 'Yale_0050626': [-0.7, 61.0, 66.0, 60.0], 'Yale_0050627': [-0.9, 88.0, 103.0, 84.0], 'Yale_0050628': [-0.3, 77.0, 72.0, 75.0], 'CMU_a_0050642': [2.0, 103.0, 98.0, 107.0], 'CMU_b_0050643': [0.5, 123.0, 112.0, 128.0], 'CMU_b_0050644': [0.2, 107.0, 110.0, 102.0], 'CMU_b_0050645': [0.4, 124.0, 128.0, 115.0], 'CMU_a_0050646': [0.5, 108.0, 100.0, 115.0], 'CMU_a_0050647': [1.2, 104.0, 97.0, 109.0], 'CMU_b_0050648': [1.7, 121.0, 132.0, 106.0], 'CMU_a_0050649': [0.6, 127.0, 121.0, 126.0], 'CMU_b_0050650': [1.7, 123.0, 119.0, 121.0], 'CMU_b_0050651': [2.7, 116.0, 108.0, 121.0], 'CMU_b_0050652': [0.9, 118.0, 113.0, 119.0], 'CMU_a_0050653': [1.6, 134.0, 131.0, 129.0], 'CMU_a_0050654': [0.9, 95.0, 99.0, 92.0], 'CMU_b_0050655': [1.2, 100.0, 89.0, 111.0], 'CMU_a_0050656': [1.4, 129.0, 125.0, 127.0], 'CMU_b_0050657': [0.5, 120.0, 112.0, 124.0], 'CMU_b_0050658': [1.2, 103.0, 108.0, 96.0], 'CMU_a_0050659': [1.2, 109.0, 111.0, 106.0], 'CMU_a_0050660': [1.0, 115.0, 116.0, 109.0], 'CMU_b_0050661': [0.4, 124.0, 128.0, 114.0], 'CMU_a_0050663': [0.5, 101.0, 94.0, 108.0], 'CMU_a_0050664': [0.5, 109.0, 107.0, 110.0], 'CMU_a_0050665': [2.0, 109.0, 111.0, 104.0], 'CMU_a_0050666': [1.7, 107.0, 101.0, 109.0], 'CMU_b_0050667': [2.9, 128.0, 122.0, 128.0], 'CMU_a_0050668': [1.0, 110.0, 109.0, 108.0], 'CMU_b_0050669': [1.6, 126.0, 123.0, 123.0], 'Leuven_1_0050682': [0.7, 128.0, 118.0, 132.0], 'Leuven_1_0050683': [0.9, 106.0, 113.0, 96.0], 'Leuven_1_0050685': [0.7, 112.0, 114.0, 108.0], 'Leuven_1_0050686': [0.2, 92.0, 88.0, 101.0], 'Leuven_1_0050687': [0.6, 124.0, 121.0, 117.0], 'Leuven_1_0050688': [0.5, 98.0, 103.0, 93.0], 'Leuven_1_0050689': [0.5, 106.0, 115.0, 93.0], 'Leuven_1_0050690': [0.6, 101.0, 95.0, 111.0], 'Leuven_1_0050691': [0.6, 146.0, 133.0, 155.0], 'Leuven_1_0050692': [0.6, 109.0, 112.0, 106.0], 'Leuven_1_0050693': [0.6, 128.0, 128.0, 120.0], 'Leuven_1_0050694': [0.2, 109.0, 103.0, 117.0], 'Leuven_1_0050695': [0.2, 101.0, 126.0, 74.0], 'Leuven_1_0050696': [0.4, 121.0, 118.0, 119.0], 'Leuven_1_0050697': [0.2, 101.0, 99.0, 106.0], 'Leuven_1_0050698': [1.4, 109.0, 107.0, 113.0], 'Leuven_1_0050699': [0.5, 104.0, 114.0, 92.0], 'Leuven_1_0050700': [0.7, 128.0, 110.0, 149.0], 'Leuven_1_0050701': [0.1, 109.0, 131.0, 84.0], 'Leuven_1_0050702': [0.1, 100.0, 110.0, 88.0], 'Leuven_1_0050703': [1.5, 108.0, 112.0, 103.0], 'Leuven_1_0050704': [1.5, 119.0, 121.0, 113.0], 'Leuven_1_0050705': [0.9, 111.0, 113.0, 107.0], 'Leuven_1_0050706': [0.6, 134.0, 136.0, 123.0], 'Leuven_1_0050707': [0.6, 113.0, 121.0, 101.0], 'Leuven_1_0050708': [1.9, 89.0, 97.0, 84.0], 'Leuven_1_0050709': [1.0, 116.0, 121.0, 107.0], 'Leuven_1_0050710': [1.2, 106.0, 106.0, 107.0], 'Leuven_1_0050711': [0.2, 126.0, 123.0, 121.0], 'Leuven_2_0050722': [-0.4, 86.3430338094494, 86.0, 89.0], 'Leuven_2_0050723': [-0.4, 98.44838931117398, 97.0, 100.0], 'Leuven_2_0050724': [-0.2, 105.05131049393286, 103.0, 106.0], 'Leuven_2_0050725': [-0.4, 117.32356681929036, 105.0, 126.0], 'Leuven_2_0050726': [-0.1, 119.65753191969092, 130.0, 106.0], 'Leuven_2_0050727': [-0.4, 118.12734110773609, 122.0, 111.0], 'Leuven_2_0050728': [-0.2, 113.76248272448194, 116.0, 109.0], 'Leuven_2_0050730': [-0.6, 111.05762690489712, 111.0, 109.0], 'Leuven_2_0050731': [-0.1, 119.20928343557004, 124.0, 111.0], 'Leuven_2_0050732': [-0.6, 125.86783822620653, 127.0, 120.0], 'Leuven_2_0050733': [-0.3, 119.20928343557004, 124.0, 111.0], 'Leuven_2_0050735': [-0.3, 103.37276339430431, 103.0, 103.0], 'Leuven_2_0050736': [-0.4, 116.5044276159852, 119.0, 111.0], 'Leuven_2_0050737': [-0.6, 112.02830201697577, 119.0, 103.0], 'Leuven_2_0050738': [-0.5, 114.73315783656061, 124.0, 103.0], 'Leuven_2_0050739': [-0.2, 112.02830201697577, 119.0, 103.0], 'Leuven_2_0050740': [-0.6, 112.99897712905442, 127.0, 97.0], 'Leuven_2_0050741': [-0.4, 118.18297471561372, 119.0, 114.0], 'Leuven_2_0050742': [-0.0, 113.65121550872666, 122.0, 103.0], 'Leuven_2_0050743': [-0.4, 81.19294567370873, 62.0, 103.0], 'Leuven_2_0050744': [-0.6, 96.71420860366781, 100.0, 94.0], 'Leuven_2_0050745': [-0.5, 93.86099649640927, 103.0, 86.0], 'Leuven_2_0050746': [-0.4, 96.45140547913904, 84.0, 109.0], 'Leuven_2_0050747': [-0.4, 94.77285837951051, 84.0, 106.0], 'Leuven_2_0050748': [-0.4, 106.13325282176679, 105.0, 106.0], 'Leuven_2_0050749': [-0.2, 84.96119928516828, 70.0, 102.0], 'Leuven_2_0050750': [-0.0, 64.63000910893392, 50.0, 85.0], 'Leuven_2_0050751': [-0.6, 130.95911313296978, 124.0, 132.0], 'Leuven_2_0050752': [-0.3, 105.2553003894842, 92.0, 117.0], 'Leuven_2_0050753': [-0.3, 80.33671739848513, 78.0, 86.0], 'Leuven_2_0050754': [-0.6, 96.76984221154545, 97.0, 97.0], 'Leuven_2_0050755': [-0.6, 99.2150745277013, 116.0, 83.0], 'Leuven_2_0050756': [-0.3, 103.57675328985566, 92.0, 114.0], 'Leuven_2_0050757': [-0.3, 74.33040098752087, 70.0, 83.0], 'KKI_0050772': [-0.5, 98.0, 99.58508682685027, 96.91583600255925], 'KKI_0050773': [-0.8, 124.0, 121.04186021906602, 122.09402306218362], 'KKI_0050774': [-0.8, 124.0, 121.04186021906602, 122.09402306218362], 'KKI_0050775': [-0.8, 101.0, 102.06086837210596, 99.82101143251589], 'KKI_0050776': [-1.0, 102.0, 102.88612888719118, 100.78940324250144], 'KKI_0050777': [-1.1, 125.0, 121.86712073415124, 123.06241487216919], 'KKI_0050778': [-0.9, 122.0, 119.39133918889556, 120.15723944221254], 'KKI_0050779': [-1.0, 105.0, 105.36191043244682, 103.69457867245812], 'KKI_0050780': [-0.9, 117.0, 115.26503661346949, 115.31528039228475], 'KKI_0050781': [-1.0, 120.0, 117.74081815872515, 118.22045582224142], 'KKI_0050782': [-0.9, 108.0, 107.83769197770249, 106.59975410241478], 'KKI_0050783': [-0.8, 121.0, 118.56607867381037, 119.18884763222695], 'KKI_0050784': [-1.1, 115.0, 113.61451558329904, 113.37849677231364], 'KKI_0050785': [-0.8, 102.0, 102.88612888719118, 100.78940324250144], 'KKI_0050786': [-1.0, 112.0, 111.13873403804335, 110.473321342357], 'KKI_0050787': [-0.8, 117.0, 115.26503661346949, 115.31528039228475], 'KKI_0050788': [-0.8, 125.0, 121.86712073415124, 123.06241487216919], 'KKI_0050789': [-1.0, 114.0, 112.78925506821383, 112.41010496232808], 'KKI_0050790': [-1.0, 100.0, 101.23560785702071, 98.85261962253036], 'KKI_0050791': [-0.9, 69.0, 75.65253188937888, 68.83247351297817], 'KKI_0050792': [-1.1, 130.0, 125.99342330957734, 127.90437392209695], 'KKI_0050793': [-1.1, 114.0, 112.78925506821383, 112.41010496232808], 'KKI_0050794': [-0.9, 90.0, 92.98300270616849, 89.16870152267482], 'KKI_0050795': [-1.1, 92.0, 94.63352373633896, 91.10548514264592], 'KKI_0050796': [-0.6, 88.0, 91.33248167599804, 87.23191790270373], 'KKI_0050797': [-0.6, 90.0, 92.98300270616849, 89.16870152267482], 'KKI_0050798': [-0.9, 102.0, 102.88612888719118, 100.78940324250144], 'KKI_0050799': [-0.7, 84.0, 88.03143961565716, 83.3583506627615], 'KKI_0050800': [-0.7, 77.0, 82.25461601006066, 76.57960799286259], 'KKI_0050801': [-0.7, 131.0, 126.81868382466259, 128.87276573208248], 'KKI_0050802': [-1.1, 104.0, 104.53664991736161, 102.72618686247255], 'KKI_0050803': [-1.1, 88.0, 91.33248167599804, 87.23191790270373], 'KKI_0050804': [-1.1, 78.0, 83.07987652514583, 77.54799980284818], 'KKI_0050807': [-0.8, 84.0, 88.03143961565716, 83.3583506627615], 'KKI_0050812': [-0.9, 121.0, 118.56607867381037, 119.18884763222695], 'KKI_0050814': [-1.1, 108.0, 107.83769197770249, 106.59975410241478], 'KKI_0050815': [-0.8, 105.0, 105.36191043244682, 103.69457867245812], 'KKI_0050816': [-0.9, 119.0, 116.91555764363993, 117.25206401225586], 'KKI_0050817': [-0.9, 119.0, 116.91555764363993, 117.25206401225586], 'KKI_0050818': [-0.7, 98.0, 99.58508682685027, 96.91583600255925], 'KKI_0050819': [-0.9, 101.0, 102.06086837210596, 99.82101143251589], 'KKI_0050820': [-1.0, 108.0, 107.83769197770249, 106.59975410241478], 'KKI_0050821': [-0.7, 114.0, 112.78925506821383, 112.41010496232808], 'KKI_0050822': [-0.6, 98.0, 99.58508682685027, 96.91583600255925], 'KKI_0050823': [-0.7, 120.0, 117.74081815872515, 118.22045582224142], 'KKI_0050824': [-0.8, 109.0, 108.66295249278771, 107.56814591240033], 'KKI_0050825': [-1.1, 89.0, 92.15774219108329, 88.20030971268926], 'KKI_0050826': [-0.8, 114.0, 112.78925506821383, 112.41010496232808], 'NYU_0050952': [-1.0, 134.0, 139.0, 120.0], 'NYU_0050954': [-0.3, 78.0, 79.0, 80.0], 'NYU_0050955': [-0.5, 93.0, 96.0, 91.0], 'NYU_0050956': [-0.4, 115.0, 115.0, 110.0], 'NYU_0050957': [-0.3, 79.0, 84.0, 79.0], 'NYU_0050958': [-0.8, 101.0, 109.0, 92.0], 'NYU_0050959': [0.7, 118.0, 129.0, 104.0], 'NYU_0050960': [2.7, 114.0, 108.0, 119.0], 'NYU_0050961': [1.0, 92.0, 89.0, 96.0], 'NYU_0050962': [0.9, 98.0, 108.0, 88.0], 'NYU_0050964': [-0.5, 106.0, 108.0, 101.0], 'NYU_0050965': [-1.0, 132.0, 119.0, 136.0], 'NYU_0050966': [-0.2, 108.0, 100.0, 115.0], 'NYU_0050967': [-0.9, 76.0, 84.0, 72.0], 'NYU_0050968': [-0.9, 108.0, 106.0, 109.0], 'NYU_0050969': [-1.2, 129.0, 108.0, 146.0], 'NYU_0050970': [-1.0, 99.0, 99.0, 99.0], 'NYU_0050972': [-0.4, 100.0, 100.0, 100.0], 'NYU_0050973': [0.1, 112.0, 113.0, 108.0], 'NYU_0050974': [-0.7, 90.0, 99.0, 85.0], 'NYU_0050976': [-0.3, 127.0, 117.0, 132.0], 'NYU_0050977': [-1.2, 142.0, 129.0, 145.0], 'NYU_0050978': [-0.9, 142.0, 126.0, 149.0], 'NYU_0050979': [-1.0, 114.0, 99.0, 129.0], 'NYU_0050981': [-0.5, 100.0, 100.0, 99.0], 'NYU_0050982': [-0.9, 119.0, 121.0, 106.0], 'NYU_0050983': [-0.8, 117.0, 132.0, 101.0], 'NYU_0050984': [-0.5, 98.0, 114.0, 83.0], 'NYU_0050985': [-0.5, 90.0, 84.0, 100.0], 'NYU_0050986': [-1.1, 102.0, 108.0, 115.0], 'NYU_0050987': [-1.1, 105.0, 93.0, 118.0], 'NYU_0050988': [-0.4, 112.0, 113.0, 115.0], 'NYU_0050989': [-0.9, 84.0, 77.0, 94.0], 'NYU_0050990': [-0.4, 131.0, 126.0, 129.0], 'NYU_0050991': [-0.8, 100.0, 97.0, 104.0], 'NYU_0050992': [-0.8, 118.0, 106.0, 129.0], 'NYU_0050993': [-0.8, 101.0, 98.0, 105.0], 'NYU_0050994': [-0.2, 102.0, 95.0, 108.0], 'NYU_0050995': [-0.0, 109.0, 98.0, 119.0], 'NYU_0050996': [-0.2, 100.0, 89.0, 112.0], 'NYU_0050997': [-0.1, 123.0, 131.0, 109.0], 'NYU_0050998': [-0.5, 91.0, 92.0, 92.0], 'NYU_0050999': [-0.3, 95.0, 102.0, 88.0], 'NYU_0051000': [-0.5, 78.0, 74.0, 88.0], 'NYU_0051001': [-0.8, 87.0, 85.0, 94.0], 'NYU_0051002': [-1.2, 90.0, 99.0, 84.0], 'NYU_0051003': [-1.1, 120.0, 106.0, 133.0], 'NYU_0051006': [-0.7, 103.0, 101.0, 104.0], 'NYU_0051007': [-0.6, 108.0, 111.0, 103.0], 'NYU_0051008': [-0.6, 128.0, 125.0, 126.0], 'NYU_0051009': [-0.7, 86.0, 88.0, 86.0], 'NYU_0051010': [-1.1, 148.0, 137.0, 147.0], 'NYU_0051011': [-1.0, 104.0, 89.0, 119.0], 'NYU_0051012': [-0.9, 96.0, 88.0, 103.0], 'NYU_0051013': [-1.2, 100.0, 94.0, 107.0], 'NYU_0051014': [-0.8, 106.0, 89.0, 125.0], 'NYU_0051015': [1.6, 105.0, 105.0, 103.0], 'NYU_0051016': [0.7, 134.0, 136.0, 124.0], 'NYU_0051017': [0.6, 137.0, 136.0, 129.0], 'NYU_0051018': [0.4, 116.0, 115.0, 114.0], 'NYU_0051019': [0.7, 108.0, 107.0, 109.0], 'NYU_0051020': [1.4, 107.0, 108.0, 103.0], 'NYU_0051021': [0.8, 112.0, 120.0, 101.0], 'NYU_0051023': [0.4, 115.0, 108.0, 119.0], 'NYU_0051024': [2.7, 98.0, 91.0, 103.0], 'NYU_0051025': [0.3, 94.0, 81.0, 109.0], 'NYU_0051026': [0.2, 108.0, 110.0, 103.0], 'NYU_0051027': [0.7, 102.0, 106.0, 99.0], 'NYU_0051028': [1.5, 80.0, 73.0, 92.0], 'NYU_0051029': [1.2, 107.0, 107.0, 110.0], 'NYU_0051030': [-1.2, 139.0, 129.0, 140.0], 'NYU_0051032': [-1.2, 93.0, 104.0, 85.0], 'NYU_0051033': [-0.9, 113.0, 102.0, 123.0], 'NYU_0051034': [-0.8, 109.0, 112.0, 103.0], 'NYU_0051035': [-0.8, 100.0, 95.0, 106.0], 'NYU_0051036': [-1.1, 101.0, 102.0, 99.0], 'NYU_0051038': [-1.1, 134.0, 143.0, 117.0], 'NYU_0051039': [-1.1, 136.0, 131.0, 133.0], 'NYU_0051040': [-1.1, 129.0, 132.0, 120.0], 'NYU_0051041': [-1.0, 100.0, 100.0, 99.0], 'NYU_0051042': [-1.0, 125.0, 117.0, 128.0], 'NYU_0051044': [-0.8, 106.0, 99.0, 111.0], 'NYU_0051045': [-0.7, 80.0, 85.0, 79.0], 'NYU_0051046': [-0.7, 103.0, 115.0, 91.0], 'NYU_0051047': [-0.6, 123.0, 119.0, 119.0], 'NYU_0051048': [-0.6, 124.0, 117.0, 126.0], 'NYU_0051049': [-0.5, 104.0, 108.0, 99.0], 'NYU_0051050': [-0.5, 100.0, 99.0, 101.0], 'NYU_0051051': [-0.4, 115.0, 109.0, 119.0], 'NYU_0051052': [-0.4, 117.0, 108.0, 124.0], 'NYU_0051053': [-0.3, 123.0, 119.0, 120.0], 'NYU_0051054': [-0.3, 132.0, 132.0, 124.0], 'NYU_0051055': [-0.1, 113.0, 118.0, 106.0], 'NYU_0051056': [0.0, 98.0, 103.0, 92.0], 'NYU_0051057': [0.6, 113.0, 109.0, 114.0], 'NYU_0051058': [0.6, 112.0, 113.0, 107.0], 'NYU_0051059': [0.7, 113.0, 113.0, 109.0], 'NYU_0051060': [0.7, 118.0, 118.0, 114.0], 'NYU_0051061': [1.2, 109.0, 115.0, 102.0], 'NYU_0051062': [1.3, 91.0, 98.0, 87.0], 'NYU_0051063': [1.5, 120.0, 118.0, 118.0], 'NYU_0051064': [-1.2, 119.0, 127.0, 108.0], 'NYU_0051065': [-0.8, 105.0, 111.0, 97.0], 'NYU_0051066': [0.2, 112.0, 117.0, 105.0], 'NYU_0051067': [0.8, 116.0, 109.0, 119.0], 'NYU_0051068': [1.8, 104.0, 104.0, 103.0], 'NYU_0051069': [-1.1, 118.0, 115.0, 117.0], 'NYU_0051070': [-1.2, 109.0, 112.0, 103.0], 'NYU_0051071': [-0.8, 80.0, 92.0, 72.0], 'NYU_0051072': [-0.6, 121.0, 120.0, 117.0], 'NYU_0051073': [-0.6, 126.0, 109.0, 137.0], 'NYU_0051074': [-0.5, 123.0, 117.0, 124.0], 'NYU_0051075': [-0.4, 109.0, 121.0, 96.0], 'NYU_0051076': [-0.0, 124.0, 120.0, 120.0], 'NYU_0051077': [0.0, 109.0, 113.0, 104.0], 'NYU_0051078': [-1.3, 114.0, 113.0, 111.0], 'NYU_0051079': [-1.2, 129.0, 120.0, 131.0], 'NYU_0051080': [-1.1, 110.0, 117.0, 102.0], 'NYU_0051081': [-1.0, 98.0, 88.0, 108.0], 'NYU_0051082': [-1.0, 119.0, 127.0, 108.0], 'NYU_0051083': [-1.0, 100.0, 91.0, 109.0], 'NYU_0051084': [-1.0, 118.0, 127.0, 106.0], 'NYU_0051085': [-0.9, 101.0, 109.0, 93.0], 'NYU_0051086': [-0.8, 119.0, 118.0, 116.0], 'NYU_0051087': [-0.8, 111.0, 102.0, 119.0], 'NYU_0051088': [-0.8, 121.0, 120.0, 117.0], 'NYU_0051089': [-0.8, 138.0, 131.0, 135.0], 'NYU_0051090': [-0.7, 107.0, 105.0, 107.0], 'NYU_0051091': [-0.7, 142.0, 141.0, 133.0], 'NYU_0051093': [-0.7, 116.0, 113.0, 116.0], 'NYU_0051094': [-0.6, 107.0, 112.0, 99.0], 'NYU_0051095': [-0.6, 123.0, 134.0, 106.0], 'NYU_0051096': [-0.5, 101.0, 95.0, 106.0], 'NYU_0051097': [-0.4, 126.0, 126.0, 119.0], 'NYU_0051098': [-0.4, 134.0, 131.0, 129.0], 'NYU_0051099': [-0.4, 87.0, 93.0, 86.0], 'NYU_0051100': [-0.3, 91.0, 102.0, 83.0], 'NYU_0051101': [-0.3, 104.0, 116.0, 92.0], 'NYU_0051102': [-0.3, 110.0, 119.0, 100.0], 'NYU_0051103': [-0.3, 126.0, 115.0, 132.0], 'NYU_0051104': [-0.2, 104.0, 96.0, 110.0], 'NYU_0051105': [-0.2, 83.0, 80.0, 89.0], 'NYU_0051106': [-0.2, 85.0, 108.0, 67.0], 'NYU_0051107': [-0.2, 107.0, 112.0, 99.0], 'NYU_0051109': [-0.1, 109.0, 109.0, 106.0], 'NYU_0051110': [-0.1, 125.0, 122.0, 120.0], 'NYU_0051111': [-0.1, 113.0, 114.0, 109.0], 'NYU_0051112': [0.3, 129.0, 127.0, 125.0], 'NYU_0051113': [0.7, 109.0, 113.0, 103.0], 'NYU_0051114': [0.7, 123.0, 122.0, 118.0], 'NYU_0051116': [1.0, 115.0, 109.0, 118.0], 'NYU_0051117': [1.4, 103.0, 104.0, 102.0], 'NYU_0051118': [1.5, 122.0, 115.0, 124.0], 'NYU_0051121': [-0.8, 113.0, 103.0, 121.0], 'NYU_0051122': [-0.6, 111.0, 102.0, 119.0], 'NYU_0051123': [-0.6, 130.0, 126.0, 128.0], 'NYU_0051124': [-0.5, 124.0, 121.0, 120.0], 'NYU_0051126': [-0.1, 81.0, 83.0, 83.0], 'NYU_0051127': [-0.1, 91.0, 88.0, 96.0], 'NYU_0051128': [-0.0, 125.0, 121.0, 123.0], 'NYU_0051129': [0.1, 102.0, 111.0, 92.0], 'NYU_0051130': [0.3, 107.0, 97.0, 117.0], 'NYU_0051131': [0.3, 119.0, 112.0, 123.0], 'Trinity_0051132': [1.0, 119.0, 113.0, 121.0], 'Trinity_0051133': [-0.6, 91.0, 100.0, 84.0], 'Trinity_0051134': [-0.0, 89.0, 81.0, 101.0], 'Trinity_0051135': [-0.4, 101.0, 95.0, 108.0], 'Trinity_0051136': [-0.5, 126.0, 125.0, 120.0], 'Trinity_0051137': [-0.5, 131.0, 137.0, 120.0], 'Trinity_0051138': [-0.5, 99.0, 94.0, 103.0], 'Trinity_0051139': [0.3, 112.0, 115.0, 112.0], 'Trinity_0051140': [-0.2, 128.0, 132.0, 121.0], 'Trinity_0051141': [-0.1, 97.0, 91.0, 102.0], 'Trinity_0051142': [-0.3, 104.0, 101.0, 105.0], 'NYU_0051146': [0.4, 106.0, 96.0, 116.0], 'NYU_0051147': [0.4, 129.0, 129.0, 111.0], 'NYU_0051148': [0.4, 107.0, 108.0, 104.0], 'NYU_0051149': [0.4, 113.0, 107.0, 118.0], 'NYU_0051150': [0.7, 132.0, 130.0, 127.0], 'NYU_0051151': [0.8, 122.0, 121.0, 117.0], 'NYU_0051152': [0.8, 139.0, 140.0, 129.0], 'NYU_0051153': [1.1, 114.0, 117.0, 108.0], 'NYU_0051154': [1.6, 104.0, 103.0, 104.0], 'NYU_0051155': [1.7, 104.0, 106.0, 100.0], 'NYU_0051156': [0.5, 120.0, 118.0, 119.0], 'NYU_0051159': [-0.5, 118.0, 122.0, 109.0], 'Stanford_0051160': [-1.1, 98.0, 97.0, 99.0], 'Stanford_0051161': [-1.0, 94.0, 95.0, 95.0], 'Stanford_0051162': [-1.0, 111.0, 106.0, 116.0], 'Stanford_0051163': [-1.0, 137.0, 137.0, 128.0], 'Stanford_0051164': [-1.1, 105.0, 88.0, 124.0], 'Stanford_0051165': [-0.6, 78.0, 72.0, 89.0], 'Stanford_0051166': [-1.1, 100.0, 93.0, 106.0], 'Stanford_0051167': [-0.6, 98.0, 96.0, 99.0], 'Stanford_0051168': [-0.7, 112.0, 108.0, 114.0], 'Stanford_0051169': [-0.9, 127.0, 131.0, 117.0], 'Stanford_0051170': [-0.8, 97.0, 86.0, 109.0], 'Stanford_0051171': [-0.5, 141.0, 143.0, 129.0], 'Stanford_0051172': [-1.0, 127.0, 149.0, 101.0], 'Stanford_0051173': [-1.2, 124.0, 113.0, 129.0], 'Stanford_0051174': [-0.9, 114.0, 106.0, 119.0], 'Stanford_0051175': [-1.1, 107.0, 99.0, 111.0], 'Stanford_0051177': [-0.7, 113.0, 117.0, 106.0], 'Stanford_0051178': [-0.7, 123.0, 121.0, 118.0], 'Stanford_0051179': [-0.7, 97.0, 101.0, 93.0], 'Stanford_0051180': [-1.2, 136.0, 144.0, 119.0], 'Stanford_0051181': [-1.0, 121.0, 122.0, 115.0], 'Stanford_0051182': [-0.8, 93.0, 88.0, 99.0], 'Stanford_0051183': [-1.1, 98.0, 103.0, 93.0], 'Stanford_0051184': [-1.0, 124.0, 98.0, 145.0], 'Stanford_0051185': [-1.1, 119.0, 107.0, 129.0], 'Stanford_0051186': [-1.1, 98.0, 88.0, 107.0], 'Stanford_0051187': [-1.1, 105.0, 115.0, 93.0], 'Stanford_0051188': [-1.0, 80.0, 83.0, 81.0], 'Stanford_0051189': [-1.0, 123.0, 136.0, 105.0], 'Stanford_0051190': [-1.0, 123.0, 107.0, 135.0], 'Stanford_0051191': [-0.9, 79.0, 67.0, 99.0], 'Stanford_0051192': [-0.8, 124.0, 130.0, 112.0], 'Stanford_0051193': [-0.9, 122.0, 132.0, 107.0], 'Stanford_0051194': [-0.6, 118.0, 106.0, 129.0], 'Stanford_0051195': [-0.7, 110.0, 115.0, 103.0], 'Stanford_0051196': [-0.6, 117.0, 131.0, 101.0], 'Stanford_0051197': [-0.6, 113.0, 118.0, 106.0], 'Stanford_0051198': [-0.6, 114.0, 109.0, 115.0], 'Stanford_0051199': [-0.6, 125.0, 125.0, 119.0], 'UCLA_1_0051201': [-0.4, 104.0, 98.0, 109.0], 'UCLA_1_0051202': [-0.7, 98.0, 110.0, 86.0], 'UCLA_1_0051203': [-0.5, 103.0, 91.0, 116.0], 'UCLA_1_0051204': [-0.3, 98.0, 110.0, 90.0], 'UCLA_1_0051205': [0.1, 102.0, 105.0, 99.0], 'UCLA_1_0051206': [-0.2, 102.0, 98.0, 104.0], 'UCLA_1_0051207': [-0.4, 112.0, 100.0, 123.0], 'UCLA_1_0051208': [-0.0, 113.0, 116.0, 104.0], 'UCLA_1_0051209': [-0.9, 128.0, 119.0, 131.0], 'UCLA_1_0051210': [-0.1, 92.0, 99.0, 91.0], 'UCLA_1_0051211': [-0.7, 94.0, 102.0, 94.0], 'UCLA_1_0051212': [-0.2, 100.0, 100.0, 97.0], 'UCLA_1_0051213': [-1.0, 98.0, 104.0, 95.0], 'UCLA_1_0051214': [-0.8, 109.0, 102.0, 108.0], 'UCLA_1_0051215': [-0.7, 113.0, 119.0, 104.0], 'UCLA_1_0051216': [-0.8, 107.0, 89.0, 112.0], 'UCLA_1_0051217': [-0.2, 104.0, 132.0, 89.0], 'UCLA_1_0051218': [-0.0, 111.0, 118.0, 103.0], 'UCLA_1_0051219': [-0.4, 79.0, 85.0, 81.0], 'UCLA_1_0051220': [-0.3, 99.0, 121.0, 90.0], 'UCLA_1_0051221': [-0.3, 75.0, 98.0, 73.0], 'UCLA_1_0051222': [-0.7, 113.0, 114.0, 106.0], 'UCLA_1_0051223': [-0.5, 119.0, 96.0, 121.0], 'UCLA_1_0051224': [-0.5, 112.0, 105.0, 119.0], 'UCLA_1_0051225': [-0.6, 111.0, 119.0, 102.0], 'UCLA_1_0051226': [-1.0, 102.0, 99.0, 104.0], 'UCLA_1_0051227': [-0.3, 95.0, 87.0, 101.0], 'UCLA_1_0051228': [-1.1, 95.0, 91.0, 98.0], 'UCLA_1_0051229': [-0.3, 95.0, 115.0, 77.0], 'UCLA_1_0051230': [0.1, 127.0, 132.0, 116.0], 'UCLA_1_0051231': [-0.8, 103.0, 104.0, 98.0], 'UCLA_1_0051234': [-0.8, 87.0, 93.0, 89.0], 'UCLA_1_0051235': [-0.8, 132.0, 130.0, 122.0], 'UCLA_1_0051236': [-0.6, 89.0, 85.0, 94.0], 'UCLA_1_0051237': [0.0, 100.0, 112.0, 94.0], 'UCLA_1_0051238': [-0.8, 73.0, 67.0, 82.0], 'UCLA_1_0051239': [-0.4, 86.0, 98.0, 83.0], 'UCLA_1_0051240': [-0.3, 121.0, 106.0, 120.0], 'UCLA_1_0051241': [-0.8, 95.0, 87.0, 105.0], 'UCLA_1_0051248': [-0.4, 110.0, 99.0, 110.0], 'UCLA_1_0051249': [-1.1, 107.0, 112.0, 103.0], 'UCLA_1_0051250': [-0.3, 110.0, 108.0, 110.0], 'UCLA_1_0051251': [-0.6, 96.0, 86.0, 107.0], 'UCLA_1_0051252': [-0.8, 116.0, 118.0, 110.0], 'UCLA_1_0051253': [-0.7, 109.0, 108.0, 110.0], 'UCLA_1_0051254': [-0.3, 113.0, 107.0, 117.0], 'UCLA_1_0051255': [-0.3, 88.0, 88.0, 93.0], 'UCLA_1_0051256': [-0.2, 108.0, 109.0, 106.0], 'UCLA_1_0051257': [-0.5, 106.0, 105.0, 105.0], 'UCLA_1_0051258': [-0.7, 90.0, 92.0, 91.0], 'UCLA_1_0051260': [-0.4, 95.0, 117.0, 76.0], 'UCLA_1_0051261': [0.1, 95.0, 100.0, 90.0], 'UCLA_1_0051262': [-0.7, 97.0, 97.0, 96.0], 'UCLA_1_0051263': [-0.1, 87.0, 89.0, 89.0], 'UCLA_1_0051264': [-0.4, 107.0, 109.0, 102.0], 'UCLA_1_0051265': [-0.1, 108.0, 108.0, 106.0], 'UCLA_1_0051266': [-0.4, 108.0, 107.0, 108.0], 'UCLA_1_0051267': [-0.7, 118.0, 119.0, 112.0], 'UCLA_1_0051268': [0.1, 105.0, 99.0, 109.0], 'UCLA_1_0051269': [-0.3, 94.0, 95.0, 95.0], 'UCLA_1_0051271': [-0.6, 113.0, 121.0, 102.0], 'UCLA_1_0051272': [-0.5, 84.0, 94.0, 77.0], 'UCLA_1_0051273': [-0.5, 104.0, 109.0, 98.0], 'UCLA_1_0051274': [-0.8, 125.0, 115.0, 129.0], 'UCLA_1_0051275': [-0.2, 106.0, 109.0, 99.0], 'UCLA_1_0051276': [-0.4, 105.0, 98.0, 109.0], 'UCLA_1_0051277': [-0.6, 126.0, 119.0, 128.0], 'UCLA_1_0051278': [-1.0, 109.0, 121.0, 97.0], 'UCLA_1_0051279': [-0.4, 106.0, 99.0, 109.0], 'UCLA_1_0051280': [-0.9, 109.0, 123.0, 96.0], 'UCLA_1_0051281': [-0.6, 108.0, 115.0, 100.0], 'UCLA_1_0051282': [-0.5, 91.0, 88.0, 96.0], 'UCLA_2_0051291': [-0.1, 86.0, 83.0, 91.0], 'UCLA_2_0051292': [-0.6, 75.0, 83.0, 79.0], 'UCLA_2_0051293': [-0.5, 92.0, 102.0, 90.0], 'UCLA_2_0051294': [-0.7, 87.0, 89.0, 90.0], 'UCLA_2_0051295': [-0.9, 108.0, 95.0, 120.0], 'UCLA_2_0051296': [-0.7, 77.0, 71.0, 85.0], 'UCLA_2_0051297': [-0.3, 88.0, 98.0, 87.0], 'UCLA_2_0051298': [-0.8, 118.0, 104.0, 132.0], 'UCLA_2_0051299': [-0.3, 104.0, 106.0, 99.0], 'UCLA_2_0051300': [-0.4, 92.0, 104.0, 89.0], 'UCLA_2_0051301': [-0.5, 91.0, 108.0, 86.0], 'UCLA_2_0051302': [-0.8, 87.0, 75.0, 97.0], 'UCLA_2_0051303': [-0.5, 99.0, 95.0, 103.0], 'UCLA_2_0051304': [-0.8, 120.0, 120.0, 116.0], 'UCLA_2_0051305': [-0.5, 102.0, 105.0, 99.0], 'UCLA_2_0051306': [-0.7, 119.0, 115.0, 119.0], 'UCLA_2_0051307': [-0.6, 128.0, 127.0, 124.0], 'UCLA_2_0051308': [-0.9, 124.0, 125.0, 118.0], 'UCLA_2_0051309': [-0.6, 107.0, 116.0, 97.0], 'UCLA_2_0051311': [-0.6, 103.0, 96.0, 109.0], 'UCLA_2_0051312': [-0.5, 123.0, 123.0, 116.0], 'UCLA_2_0051313': [-0.5, 112.0, 112.0, 109.0], 'UCLA_2_0051314': [-0.5, 97.0, 106.0, 89.0], 'UCLA_2_0051315': [-0.4, 118.0, 115.0, 117.0], 'UCLA_2_0051316': [-0.5, 94.0, 87.0, 103.0], 'UCLA_2_0051317': [-0.5, 102.0, 99.0, 103.0], 'MaxMun_a_0051318': [0.2, 93.0, 95.45878425142415, 92.07387695263148], 'MaxMun_a_0051319': [2.0, 118.0, 116.09029712855468, 116.28367220227032], 'MaxMun_a_0051320': [1.7, 110.0, 109.48821300787293, 108.53653772238589], 'MaxMun_a_0051321': [3.5, 129.0, 125.16816279449213, 126.9359821121114], 'MaxMun_b_0051322': [1.5, 122.0, 119.39133918889556, 120.15723944221254], 'MaxMun_b_0051323': [1.6, 118.0, 116.09029712855468, 116.28367220227032], 'MaxMun_b_0051324': [4.3, 133.0, 128.46920485483298, 130.80954935205364], 'MaxMun_b_0051325': [1.9, 110.0, 109.48821300787293, 108.53653772238589], 'MaxMun_b_0051326': [3.1, 114.0, 112.78925506821383, 112.41010496232808], 'MaxMun_b_0051327': [5.1, 129.0, 125.16816279449213, 126.9359821121114], 'MaxMun_c_0051328': [0.6, 114.0, 103.90880686159397, 122.0], 'MaxMun_d_0051329': [0.1, 95.0, 86.00692017744697, 106.0], 'MaxMun_d_0051330': [3.5, 102.96538198384071, 106.20555404711742, 99.0], 'MaxMun_d_0051331': [2.2, 116.66372337376293, 111.27383492519624, 119.0], 'MaxMun_c_0051332': [0.7, 125.0, 119.14685398693861, 126.0], 'MaxMun_c_0051333': [0.9, 99.0, 108.63733258371232, 89.0], 'MaxMun_c_0051334': [1.2, 104.0, 96.87492173885377, 111.0], 'MaxMun_c_0051335': [0.7, 122.0, 113.98081776887082, 126.0], 'MaxMun_c_0051336': [1.1, 114.0, 111.31697821006847, 114.0], 'MaxMun_c_0051338': [1.6, 110.0, 97.02075857083692, 122.0], 'MaxMun_c_0051339': [1.5, 110.0, 109.05903701210798, 109.0], 'MaxMun_c_0051340': [1.5, 110.0, 133.13559389465007, 83.0], 'MaxMun_c_0051341': [1.5, 110.0, 107.20699417498935, 111.0], 'MaxMun_c_0051342': [0.7, 107.0, 110.37515072395537, 102.0], 'MaxMun_c_0051343': [1.6, 118.0, 110.79685515235101, 122.0], 'MaxMun_c_0051344': [0.6, 118.0, 134.87341203489314, 96.0], 'MaxMun_c_0051345': [2.2, 107.0, 91.85472235276913, 122.0], 'MaxMun_c_0051346': [1.0, 118.0, 127.46524068641862, 104.0], 'MaxMun_c_0051347': [0.9, 129.0, 126.03490227769566, 126.0], 'MaxMun_c_0051348': [-1.1, 102.0, 102.88612888719118, 100.78940324250144], 'MaxMun_d_0051349': [-0.8, 105.0, 105.36191043244682, 103.69457867245812], 'MaxMun_d_0051350': [-0.8, 79.0, 83.9051370402311, 78.5163916128337], 'MaxMun_d_0051351': [-0.8, 79.0, 83.9051370402311, 78.5163916128337], 'MaxMun_d_0051352': [-1.0, 104.0, 104.53664991736161, 102.72618686247255], 'MaxMun_d_0051353': [-1.3, 113.0, 111.96399455312861, 111.44171315234252], 'MaxMun_d_0051354': [-0.6, 101.0, 102.06086837210596, 99.82101143251589], 'MaxMun_d_0051355': [-0.8, 122.0, 119.39133918889556, 120.15723944221254], 'MaxMun_d_0051356': [-0.9, 129.0, 125.16816279449213, 126.9359821121114], 'MaxMun_d_0051357': [-0.8, 110.0, 109.48821300787293, 108.53653772238589], 'MaxMun_d_0051358': [-0.9, 113.0, 111.96399455312861, 111.44171315234252], 'MaxMun_d_0051359': [-0.5, 101.0, 102.06086837210596, 99.82101143251589], 'MaxMun_d_0051360': [-1.3, 98.0, 99.58508682685027, 96.91583600255925], 'MaxMun_d_0051361': [-0.1, 114.0, 112.78925506821383, 112.41010496232808], 'MaxMun_a_0051362': [2.0, 114.0, 112.78925506821383, 112.41010496232808], 'MaxMun_a_0051363': [1.9, 112.0, 111.13873403804335, 110.473321342357], 'MaxMun_a_0051364': [1.1, 122.0, 119.39133918889556, 120.15723944221254], 'MaxMun_a_0051365': [0.7, 105.0, 105.36191043244682, 103.69457867245812], 'MaxMun_a_0051369': [1.9, 96.0, 97.93456579667982, 94.97905238258814], 'MaxMun_a_0051370': [1.9, 95.0, 97.10930528159463, 94.01066057260256], 'MaxMun_a_0051373': [3.6, 119.0, 116.91555764363993, 117.25206401225586], 'Caltech_0051456': [4.8, 126.0, 118.0, 128.0], 'Caltech_0051457': [0.7, 107.0, 119.0, 93.0], 'Caltech_0051458': [2.8, 93.0, 80.0, 108.0], 'Caltech_0051459': [0.7, 106.0, 94.0, 118.0], 'Caltech_0051460': [2.2, 133.0, 135.0, 122.0], 'Caltech_0051461': [2.6, 99.0, 111.0, 84.0], 'Caltech_0051462': [0.4, 107.0, 102.0, 110.0], 'Caltech_0051463': [0.4, 102.0, 101.0, 103.0], 'Caltech_0051464': [0.5, 101.0, 118.0, 109.0], 'Caltech_0051465': [0.4, 96.0, 99.0, 93.0], 'Caltech_0051466': [1.3, 106.0, 111.0, 99.0], 'Caltech_0051467': [0.8, 93.0, 89.0, 99.0], 'Caltech_0051468': [0.4, 100.0, 89.0, 109.0], 'Caltech_0051469': [3.5, 104.0, 109.0, 96.0], 'Caltech_0051470': [1.5, 124.0, 127.0, 115.0], 'Caltech_0051471': [0.7, 125.0, 123.0, 119.0], 'Caltech_0051472': [0.1, 125.0, 123.0, 119.0], 'Caltech_0051473': [0.5, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'Caltech_0051474': [0.5, 100.0, 90.0, 109.0], 'Caltech_0051476': [2.8, 123.0, 127.0, 112.0], 'Caltech_0051477': [3.2, 97.0, 85.0, 109.0], 'Caltech_0051478': [0.3, 116.0, 117.0, 116.0], 'Caltech_0051479': [0.4, 111.0, 110.0, 109.0], 'Caltech_0051480': [0.5, 112.0, 109.0, 110.0], 'Caltech_0051481': [1.4, 117.0, 133.0, 99.0], 'Caltech_0051482': [0.5, 103.0, 108.0, 96.0], 'Caltech_0051483': [0.5, 124.0, 127.0, 115.0], 'Caltech_0051484': [0.8, 128.0, 135.0, 115.0], 'Caltech_0051485': [0.9, 123.0, 113.0, 127.0], 'Caltech_0051486': [0.6, 134.0, 131.0, 129.0], 'Caltech_0051487': [-0.0, 113.0, 106.0, 119.0], 'Caltech_0051488': [0.8, 107.0, 106.0, 106.0], 'Caltech_0051489': [2.1, 111.0, 118.0, 102.0], 'Caltech_0051490': [3.4, 108.0, 107.0, 108.0], 'Caltech_0051491': [4.9, 120.0, 109.0, 128.0], 'Caltech_0051492': [0.2, 117.0, 119.0, 110.0], 'Caltech_0051493': [1.5, 102.0, 101.0, 103.0], 'SBL_0051556': [0.4, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051557': [1.1, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051558': [1.2, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051559': [1.2, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051560': [1.2, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051561': [2.4, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051562': [2.5, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051563': [2.6, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051564': [2.7, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051565': [2.7, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051566': [3.0, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051567': [2.1, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051568': [2.5, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051569': [2.4, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051570': [3.1, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051571': [0.6, 106.20743096560363, 101.0, 110.0], 'SBL_0051572': [1.4, 101.8796616542679, 93.0, 110.0], 'SBL_0051573': [1.6, 115.58938611178418, 108.0, 120.0], 'SBL_0051574': [1.6, 114.52598831990947, 105.0, 121.0], 'SBL_0051575': [1.7, 107.34500690131522, 100.0, 113.0], 'SBL_0051576': [1.7, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051577': [1.7, 108.32282741312406, 108.10253434244804, 106.9140776506778], 'SBL_0051578': [2.0, 125.0, 129.0, 113.0], 'SBL_0051579': [3.0, 110.0, 109.48821300787293, 108.53653772238589], 'SBL_0051580': [3.1, 121.01764228691304, 117.0, 121.0], 'SBL_0051581': [5.8, 120.0, 119.0, 116.0], 'SBL_0051582': [1.7, 120.47667112299608, 116.0, 121.0], 'SBL_0051583': [2.2, 95.0, 105.0, 84.0], 'SBL_0051584': [4.0, 137.50640070785101, 133.0, 135.0], 'SBL_0051585': [1.2, 96.0, 99.0, 106.0], 'MaxMun_a_0051606': [1.5, 118.0, 116.09029712855468, 116.28367220227032], 'MaxMun_a_0051607': [1.1, 110.0, 109.48821300787293, 108.53653772238589]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP1zt_HDje7M"
      },
      "source": [
        "### Helper functions for computing correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a7y4oZhgje7M"
      },
      "outputs": [],
      "source": [
        "def get_label(filename):\n",
        "    assert (filename in labels)\n",
        "    return labels[filename]\n",
        "\n",
        "\n",
        "def get_corr_data(filename):\n",
        "    #print(filename)\n",
        "    for file in os.listdir(data_main_path):\n",
        "        if file.startswith(filename):\n",
        "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
        "        m = ma.masked_where(mask == 1, mask)\n",
        "        return ma.masked_where(m, corr).compressed()\n",
        "\n",
        "def get_corr_matrix(filename):\n",
        "    for file in os.listdir(data_main_path):\n",
        "        if file.startswith(filename):\n",
        "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        return corr\n",
        "\n",
        "def confusion(g_turth,predictions):\n",
        "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
        "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
        "    sensitivity = (tp)/(tp+fn)\n",
        "    specificty = (tn)/(tn+fp)\n",
        "    return accuracy,sensitivity,specificty\n",
        "\n",
        "def get_regs(samplesnames,regnum):\n",
        "    datas = []\n",
        "    for sn in samplesnames:\n",
        "        datas.append(all_corr[sn][0])\n",
        "    datas = np.array(datas)\n",
        "    avg=[]\n",
        "    for ie in range(datas.shape[1]):\n",
        "        avg.append(np.mean(datas[:,ie]))\n",
        "    avg=np.array(avg)\n",
        "    highs=avg.argsort()[-regnum:][::-1]\n",
        "    lows=avg.argsort()[:regnum][::-1]\n",
        "    regions=np.concatenate((highs,lows),axis=0)\n",
        "    return regions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_df_Lrp8je7M"
      },
      "source": [
        "## Helper fnuctions for computing correlations (Step A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_980i7pjje7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da86c7a6-4e61-4ef3-83f4-c585960f67ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:01:12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corr-computations finished\n",
            "Saving to file finished\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('./correlations_file'+p_ROI+'.pkl'):\n",
        "    pbar=pyprind.ProgBar(len(flist))\n",
        "    all_corr = {}\n",
        "    for f in flist:\n",
        "\n",
        "        lab = get_label(f)\n",
        "        all_corr[f] = (get_corr_data(f), lab)\n",
        "        pbar.update()\n",
        "\n",
        "    print('Corr-computations finished')\n",
        "\n",
        "    pickle.dump(all_corr, open('./correlations_file'+p_ROI+'.pkl', 'wb'))\n",
        "    print('Saving to file finished')\n",
        "\n",
        "else:\n",
        "    all_corr = pickle.load(open('./correlations_file'+p_ROI+'.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vFuNrLsje7N"
      },
      "source": [
        "## Computing eigenvalues and eigenvector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Putbeqefje7N",
        "outputId": "b15c4a54-70c2-40d6-f79a-9a12a130327a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:01:44\n"
          ]
        }
      ],
      "source": [
        "if p_Method==\"ASD-DiagNet\":\n",
        "    eig_data = {}\n",
        "    pbar = pyprind.ProgBar(len(flist))\n",
        "    for f in flist:\n",
        "        d = get_corr_matrix(f)\n",
        "        eig_vals, eig_vecs = np.linalg.eig(d)\n",
        "\n",
        "        for ev in eig_vecs.T:\n",
        "            np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
        "\n",
        "        sum_eigvals = np.sum(np.abs(eig_vals))\n",
        "        # Make a list of (eigenvalue, eigenvector, norm_eigval) tuples\n",
        "        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i], np.abs(eig_vals[i])/sum_eigvals)\n",
        "                     for i in range(len(eig_vals))]\n",
        "\n",
        "        # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
        "        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        eig_data[f] = {'eigvals':np.array([ep[0] for ep in eig_pairs]),\n",
        "                       'norm-eigvals':np.array([ep[2] for ep in eig_pairs]),\n",
        "                       'eigvecs':[ep[1] for ep in eig_pairs]}\n",
        "        pbar.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mUbBcVXje7N"
      },
      "source": [
        "## Calculating Eros similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9GN9yITje7N"
      },
      "outputs": [],
      "source": [
        "def norm_weights(sub_flist):\n",
        "    num_dim = len(eig_data[flist[0]]['eigvals'])\n",
        "    norm_weights = np.zeros(shape=num_dim)\n",
        "    for f in sub_flist:\n",
        "        norm_weights += eig_data[f]['norm-eigvals']\n",
        "    return norm_weights\n",
        "\n",
        "def cal_similarity(d1, d2, weights, lim=None):\n",
        "    res = 0.0\n",
        "    if lim is None:\n",
        "        weights_arr = weights.copy()\n",
        "    else:\n",
        "        weights_arr = weights[:lim].copy()\n",
        "        weights_arr /= np.sum(weights_arr)\n",
        "    for i,w in enumerate(weights_arr):\n",
        "        res += w*np.inner(d1[i], d2[i])\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmfYEmZije7N"
      },
      "source": [
        "## Defining dataset class (Step B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiJEROcRje7N"
      },
      "outputs": [],
      "source": [
        "class CC200Dataset(Dataset):\n",
        "    def __init__(self, pkl_filename=None, data=None, samples_list=None,\n",
        "                 phenotype_data=None,#new\n",
        "                 augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "                 eig_data=None, similarity_fn=None, verbose=False, regs=None):\n",
        "        self.regs = regs\n",
        "        self.phenotype_data = phenotype_data  #new\n",
        "        if pkl_filename is not None:\n",
        "            if verbose:\n",
        "                print('Loading ..!', end=' ')\n",
        "            self.data = pickle.load(open(pkl_filename, 'rb'))\n",
        "        elif data is not None:\n",
        "            self.data = data.copy()\n",
        "        else:\n",
        "            sys.stderr.write('Either PKL file or data is needed!')\n",
        "            return\n",
        "\n",
        "        #if verbose:\n",
        "        #    print ('Preprocess..!', end='  ')\n",
        "        if samples_list is None:\n",
        "            self.flist = [f for f in self.data]\n",
        "        else:\n",
        "            self.flist = [f for f in samples_list]\n",
        "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
        "\n",
        "        current_flist = np.array(self.flist.copy())\n",
        "        current_lab0_flist = current_flist[self.labels == 0]\n",
        "        current_lab1_flist = current_flist[self.labels == 1]\n",
        "        #if verbose:\n",
        "        #    print(' Num Positive : ', len(current_lab1_flist), end=' ')\n",
        "        #    print(' Num Negative : ', len(current_lab0_flist), end=' ')\n",
        "\n",
        "\n",
        "        if augmentation:\n",
        "            self.num_data = aug_factor * len(self.flist)\n",
        "            self.neighbors = {}\n",
        "            pbar = pyprind.ProgBar(len(self.flist))\n",
        "            weights = norm_weights(samples_list)#??\n",
        "            for f in self.flist:\n",
        "                label = self.data[f][1]\n",
        "                candidates = (set(current_lab0_flist) if label == 0 else set(current_lab1_flist))\n",
        "                candidates.remove(f)\n",
        "                eig_f = eig_data[f]['eigvecs']\n",
        "                sim_list = []\n",
        "                for cand in candidates:\n",
        "                    eig_cand = eig_data[cand]['eigvecs']\n",
        "                    sim = similarity_fn(eig_f, eig_cand, weights)\n",
        "                    sim_list.append((sim, cand))\n",
        "                sim_list.sort(key=lambda x: x[0], reverse=True)\n",
        "                self.neighbors[f] = [item[1] for item in sim_list[:num_neighbs]]#list(candidates)#[item[1] for item in sim_list[:num_neighbs]]\n",
        "\n",
        "        else:\n",
        "            self.num_data = len(self.flist)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index < len(self.flist):\n",
        "            fname = self.flist[index]\n",
        "            data = self.data[fname][0].copy() #get_corr_data(fname, mode=cal_mode)\n",
        "            data = data[self.regs].copy()\n",
        "            label = (self.labels[index],)\n",
        "\n",
        "            #new\n",
        "            if self.phenotype_data is not None:\n",
        "                # Retrieve phenotype data using the file identifier as key\n",
        "                pheno_vals = self.phenotype_data[fname]\n",
        "                return torch.FloatTensor(data), torch.FloatTensor(pheno_vals), torch.FloatTensor(label)\n",
        "            else:\n",
        "                return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "\n",
        "        else:\n",
        "            f1 = self.flist[index % len(self.flist)]\n",
        "            d1, y1 = self.data[f1][0], self.data[f1][1]\n",
        "            d1=d1[self.regs]\n",
        "            if len(self.neighbors[f1]) > 0:#new\n",
        "                f2 = np.random.choice(self.neighbors[f1])\n",
        "            else:\n",
        "                f2 = f1  # fallback to self if no neighbors exist\n",
        "            d2, y2 = self.data[f2][0], self.data[f2][1]\n",
        "            d2 = d2[self.regs]\n",
        "            assert y1 == y2\n",
        "            r = np.random.uniform(low=0, high=1)\n",
        "            label = (y1,)\n",
        "            data = r * d1 + (1 - r) * d2\n",
        "\n",
        "            #new check---------\n",
        "            if self.phenotype_data is not None:\n",
        "                # Use phenotype from the first sample for the augmented data\n",
        "                pheno_vals = self.phenotype_data[f1]\n",
        "                return torch.FloatTensor(data), torch.FloatTensor(pheno_vals), torch.FloatTensor(label)\n",
        "            else:\n",
        "                return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfUMb1Z2je7O"
      },
      "source": [
        "## Definig data loader function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlGMx4Lrje7O"
      },
      "outputs": [],
      "source": [
        "def get_loader(pkl_filename=None, data=None, samples_list=None,\n",
        "               batch_size=64,\n",
        "               num_workers=1, mode='train',\n",
        "               *, augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "               eig_data=None, similarity_fn=None, verbose=False, regions=None,\n",
        "               phenotype_data=None):  #new pheno data\n",
        "    \"\"\"Build and return data loader.\"\"\"\n",
        "    if mode == 'train':\n",
        "        shuffle = True\n",
        "    else:\n",
        "        shuffle = False\n",
        "        augmentation = False\n",
        "\n",
        "    dataset = CC200Dataset(pkl_filename=pkl_filename, data=data, samples_list=samples_list,\n",
        "                           augmentation=augmentation, aug_factor=aug_factor,\n",
        "                           eig_data=eig_data, similarity_fn=similarity_fn, verbose=verbose, regs=regions,\n",
        "                           phenotype_data=phenotype_data)  #new pheno data\n",
        "\n",
        "    data_loader = DataLoader(dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers)\n",
        "\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK8W0nMSje7O"
      },
      "source": [
        "## Defining Autoencoder class (Step C & D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1pd8JMdje7O",
        "outputId": "3e06a5e6-27bb-4c4b-ff23-288a1e3c9f95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MTAutoEncoder(\n",
              "  (fc_encoder): Linear(in_features=990, out_features=200, bias=True)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=204, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "class MTAutoEncoder(nn.Module):\n",
        "    def __init__(self, num_inputs=990,\n",
        "                 num_latent=200, tied=True,\n",
        "                 num_classes=2, use_dropout=False):\n",
        "        super(MTAutoEncoder, self).__init__()\n",
        "        self.tied = tied\n",
        "        self.num_latent = num_latent\n",
        "\n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "\n",
        "        if not tied:\n",
        "            self.fc_decoder = nn.Linear(num_latent, num_inputs)\n",
        "\n",
        "\n",
        "        if use_dropout:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Dropout(p=0.5),\n",
        "                nn.Linear(self.num_latent + 4, 1)#new +1,1\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(self.num_latent + 4, 1)#new +1,1\n",
        "            )\n",
        "\n",
        "    def forward(self, x, pheno_data=None, eval_classifier=False):\n",
        "      x = self.fc_encoder(x)\n",
        "      x = torch.tanh(x)\n",
        "      #print(pheno_data)\n",
        "      #print(\"=============\")\n",
        "      if eval_classifier:\n",
        "          if pheno_data is None:\n",
        "              raise ValueError(\"Classifier mode requires pheno values input\")\n",
        "\n",
        "          pheno_data = pheno_data.to(x.device)\n",
        "          combined = torch.cat((x, pheno_data), dim=1)\n",
        "          # Print the classifier input vector shape only once per cell\n",
        "          if not hasattr(self, '_printed_vector_size'):\n",
        "              print(\"Classifier input vector shape (with pheno data):\", combined.shape)\n",
        "              self._printed_vector_size = True\n",
        "          x_logit = self.classifier(combined)\n",
        "      else:\n",
        "          x_logit = None\n",
        "\n",
        "      if self.tied:\n",
        "          x = F.linear(x, self.fc_encoder.weight.t())\n",
        "      else:\n",
        "          x = self.fc_decoder(x)\n",
        "\n",
        "      return x, x_logit\n",
        "\n",
        "\n",
        "mtae = MTAutoEncoder()\n",
        "mtae\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLxPmE4kje7O"
      },
      "source": [
        "## Defining training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Glfo2XVQje7O"
      },
      "outputs": [],
      "source": [
        "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "\n",
        "    for i, (images, pheno_data, batch_y) in enumerate(train_loader):#new before batch_x now = images\n",
        "        if len(images) != batch_size:\n",
        "            continue\n",
        "\n",
        "        if p_bernoulli is not None:\n",
        "            if i == 0:\n",
        "                p_tensor = torch.ones_like(images).to(device) * p_bernoulli\n",
        "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
        "\n",
        "        images = images.to(device)\n",
        "        pheno_data = pheno_data.to(device)#new\n",
        "        batch_y = batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if mode in ['both', 'ae']:\n",
        "            if p_bernoulli is not None:\n",
        "                rec_noisy, _ = model(images * rand_bernoulli, pheno_data, False)#new\n",
        "                loss_ae = criterion_ae(rec_noisy, images) / len(images)\n",
        "            else:\n",
        "                rec, _ = model(images, pheno_data, False)#new\n",
        "                loss_ae = criterion_ae(rec, images) / len(images)\n",
        "\n",
        "        if mode in ['both', 'clf']:\n",
        "            rec_clean, logits = model(images, pheno_data, True)#new\n",
        "            loss_clf = criterion_clf(logits, batch_y)\n",
        "\n",
        "        if mode == 'both':\n",
        "            loss_total = loss_ae + lam_factor * loss_clf\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), loss_clf.detach().cpu().numpy()])\n",
        "        elif mode == 'ae':\n",
        "            loss_total = loss_ae\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), 0.0])\n",
        "        elif mode == 'clf':\n",
        "            loss_total = loss_clf\n",
        "            train_losses.append([0.0, loss_clf.detach().cpu().numpy()])\n",
        "\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return train_losses\n",
        "\n",
        "def test(model, criterion, test_loader,\n",
        "         eval_classifier=False, num_batch=None):\n",
        "    test_loss, n_test, correct = 0.0, 0, 0\n",
        "    all_predss = []\n",
        "    if eval_classifier:\n",
        "        y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i, (images, pheno_data, batch_y) in enumerate(test_loader, 1):#new\n",
        "            if num_batch is not None and i >= num_batch:\n",
        "                continue\n",
        "            images = images.to(device)\n",
        "            pheno_data_data = pheno_data.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            rec, logits = model(images, pheno_data, eval_classifier)\n",
        "            test_loss += criterion(rec, images).detach().cpu().numpy()\n",
        "            n_test += len(images)\n",
        "            if eval_classifier:\n",
        "                proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                preds = np.ones_like(proba, dtype=np.int32)\n",
        "                preds[proba < 0.5] = 0\n",
        "                all_predss.extend(preds)\n",
        "                y_arr = batch_y.cpu().numpy().astype(np.int32)\n",
        "                correct += np.sum(preds == y_arr)\n",
        "                y_true.extend(y_arr.tolist())\n",
        "                y_pred.extend(proba.tolist())\n",
        "        if eval_classifier:\n",
        "            mlp_acc, mlp_sens, mlp_spef = confusion(y_true, all_predss)\n",
        "    return mlp_acc, mlp_sens, mlp_spef\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLBvWm1Zje7O",
        "outputId": "5fb4c3f6-558a-4573-aecc-bca1643061e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "bjxpvdXUje7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c543db-2f6c-463f-fec0-3838c73c7d09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_corr:   19900\n",
            "Initial n_lat: 4975\n",
            "p_bernoulli: None\n",
            "augmentation: True aug_factor: 2 num_neighbs: 5 lim4sim: 2\n",
            "use_dropout: False \n",
            "\n",
            "Using phenotypic data from key: Pitt_0050003\n",
            "Using 10-fold cross-validation with phenotypic data for whole dataset.\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Fold result: (0.5, 1.0, 0.0)\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Fold result: (0.5, 0.0, 1.0)\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Fold result: (0.5, 0.0, 1.0)\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Fold result: (0.75, 1.0, 0.5)\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Fold result: (0.5, 1.0, 0.0)\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Fold result: (0.5, 0.0, 1.0)\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Fold result: (0.6666666666666666, 0.0, 1.0)\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n"
          ]
        }
      ],
      "source": [
        "if p_Method == \"ASD-DiagNet\" and p_mode == \"whole\":\n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "    print(\"num_corr:  \", num_corr)\n",
        "\n",
        "    start = time.time()\n",
        "    batch_size = 8\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "    num_epochs = 25\n",
        "\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = False\n",
        "\n",
        "    aug_factor = 2\n",
        "    num_neighbs = 5\n",
        "    lim4sim = 2\n",
        "    n_lat = int(num_corr / 4)\n",
        "    print(\"Initial n_lat:\", n_lat)\n",
        "    start = time.time()\n",
        "\n",
        "    print('p_bernoulli:', p_bernoulli)\n",
        "    print('augmentation:', augmentation, 'aug_factor:', aug_factor,\n",
        "          'num_neighbs:', num_neighbs, 'lim4sim:', lim4sim)\n",
        "    print('use_dropout:', use_dropout, '\\n')\n",
        "\n",
        "    # Check if phenotypic data is available and optionally print a sample vector.\n",
        "    if len(phen_dict) > 0:\n",
        "        sample_key = list(phen_dict.keys())[0]\n",
        "        pheno_vect = phen_dict[sample_key]\n",
        "\n",
        "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "    crossval_res_kol = []\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "    flist = np.array(flist)\n",
        "\n",
        "    # Optionally adjust folds based on phenotypic data (as in percenter code)\n",
        "    if len(phen_dict) > 0:\n",
        "        unique_labels, counts = np.unique(y_arr, return_counts=True)\n",
        "        new_n_splits = min(p_fold, counts.min())\n",
        "        if new_n_splits < 2:\n",
        "            print(\"Skipping due to insufficient samples per class when using phenotypic data.\")\n",
        "            # You could exit or set new_n_splits = 2 if that makes sense.\n",
        "        else:\n",
        "            print(f\"Using {new_n_splits}-fold cross-validation with phenotypic data for whole dataset.\")\n",
        "    else:\n",
        "        new_n_splits = p_fold\n",
        "\n",
        "    for rp in range(10):\n",
        "        kf = StratifiedKFold(n_splits=new_n_splits, random_state=1, shuffle=True)\n",
        "        np.random.shuffle(flist)\n",
        "        y_arr = np.array([get_label(f) for f in flist])\n",
        "        for kk, (train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "            train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "            regions_inds = get_regs(train_samples, int(num_corr / 4))\n",
        "            num_inpp = len(regions_inds)\n",
        "            n_lat = int(num_inpp / 2)\n",
        "\n",
        "            # Include phenotype_data in both train and test loaders.\n",
        "            train_loader = get_loader(data=all_corr, samples_list=train_samples,\n",
        "                                      batch_size=batch_size, mode='train',\n",
        "                                      augmentation=augmentation, aug_factor=aug_factor,\n",
        "                                      num_neighbs=num_neighbs, eig_data=eig_data,\n",
        "                                      similarity_fn=sim_function, verbose=verbose,\n",
        "                                      regions=regions_inds, phenotype_data=phen_dict)\n",
        "\n",
        "            test_loader = get_loader(data=all_corr, samples_list=test_samples,\n",
        "                                     batch_size=batch_size, mode='test',\n",
        "                                     augmentation=False, verbose=verbose,\n",
        "                                     regions=regions_inds, phenotype_data=phen_dict)\n",
        "\n",
        "            model = MTAutoEncoder(tied=True, num_inputs=num_inpp,\n",
        "                                  num_latent=n_lat, use_dropout=use_dropout)\n",
        "            model.to(device)\n",
        "            criterion_ae = nn.MSELoss(reduction='sum')\n",
        "            criterion_clf = nn.BCEWithLogitsLoss()\n",
        "            optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                                   {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                                  momentum=0.9)\n",
        "\n",
        "            for epoch in range(1, num_epochs + 1):\n",
        "                if epoch <= 20:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                else:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "            res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
        "            crossval_res_kol.append(res_mlp)\n",
        "        print(\"Averages over folds:\")\n",
        "        print(np.mean(np.array(crossval_res_kol), axis=0))\n",
        "        finish = time.time()\n",
        "        print(\"Elapsed time:\", finish - start)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_ROI = \"cc200\"\n",
        "p_fold = 5  # Use 5-fold for intra-site evaluation\n",
        "p_center = \"Stanford\"\n",
        "p_mode = \"percenter\"  # Change to \"percenter\" for intra-site evaluation\n",
        "p_augmentation = True\n",
        "p_Method = \"ASD-DiagNet\""
      ],
      "metadata": {
        "id": "lHRl9EzZWivb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if p_Method == \"ASD-DiagNet\" and p_mode == \"percenter\":\n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "\n",
        "    if len(phen_dict) > 0:\n",
        "        sample_key = list(phen_dict.keys())[0]\n",
        "        pheno_vect = phen_dict[sample_key]\n",
        "\n",
        "    flist = os.listdir(data_main_path)\n",
        "\n",
        "    for f in range(len(flist)):\n",
        "        flist[f] = get_key(flist[f])\n",
        "\n",
        "    centers_dict = {}\n",
        "    for f in flist:\n",
        "        key = f.split('_')[0]\n",
        "\n",
        "        if key not in centers_dict:\n",
        "            centers_dict[key] = []\n",
        "        centers_dict[key].append(f)\n",
        "\n",
        "    flist = np.array(centers_dict[p_center])\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "\n",
        "    # Determine the number of splits.\n",
        "    if len(phen_dict) > 0:\n",
        "        unique_labels, counts = np.unique(y_arr, return_counts=True)\n",
        "        new_n_splits = min(p_fold, counts.min())\n",
        "        if new_n_splits < 2:\n",
        "            print(f\"Skipping center {p_center} due to insufficient samples in one class.\")\n",
        "            # Optionally, exit or continue to the next center.\n",
        "        else:\n",
        "            print(f\"Using {new_n_splits}-fold cross-validation for center {p_center}.\")\n",
        "    else:\n",
        "        new_n_splits = p_fold\n",
        "        print(f\"Using {new_n_splits}-fold cross-validation for center {p_center}.\")\n",
        "\n",
        "    flist = np.array(centers_dict[p_center])\n",
        "\n",
        "    start =time.time()\n",
        "    #flist = np.array(sorted(os.listdir(data_main_path)))\n",
        "    batch_size = 8\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "\n",
        "    num_epochs = 25\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = False\n",
        "\n",
        "    aug_factor = 2\n",
        "    num_neighbs = 5\n",
        "    lim4sim = 2\n",
        "    n_lat = int(num_corr / 4)\n",
        "\n",
        "    print('p_bernoulli: ', p_bernoulli)\n",
        "    print('augmentation: ', augmentation, 'aug_factor: ', aug_factor,\n",
        "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "    print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "    all_rp_res = []\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "\n",
        "    # Only continue if we have enough splits.\n",
        "    if len(phen_dict) > 0 and new_n_splits < 2:\n",
        "        pass\n",
        "    else:\n",
        "        for rp in range(10):\n",
        "            print(\"========================\")\n",
        "            crossval_res_kol = []\n",
        "            start_rp = time.time()\n",
        "            kf = StratifiedKFold(n_splits=new_n_splits)\n",
        "            for kk, (train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "\n",
        "                train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "                verbose = (True if (kk == 0) else False)\n",
        "\n",
        "                regions_inds = get_regs(train_samples, int(num_corr/4))\n",
        "                num_inpp = len(regions_inds)\n",
        "                n_lat = int(num_inpp/2)\n",
        "                num_inpp = len(regions_inds)\n",
        "                if len(phen_dict) > 0:\n",
        "                    train_loader = get_loader(data=all_corr, samples_list=train_samples,\n",
        "                                              batch_size=batch_size, mode='train',\n",
        "                                              augmentation=augmentation, aug_factor=aug_factor,\n",
        "                                              num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function,\n",
        "                                              verbose=verbose, regions=regions_inds, phenotype_data=phen_dict)\n",
        "\n",
        "                    test_loader = get_loader(data=all_corr, samples_list=test_samples,\n",
        "                                             batch_size=batch_size, mode='test', augmentation=False,\n",
        "                                             verbose=verbose, regions=regions_inds, phenotype_data=phen_dict)\n",
        "                else:\n",
        "                    train_loader = get_loader(data=all_corr, samples_list=train_samples,\n",
        "                                              batch_size=batch_size, mode='train',\n",
        "                                              augmentation=augmentation, aug_factor=aug_factor,\n",
        "                                              num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function,\n",
        "                                              verbose=verbose, regions=regions_inds)\n",
        "\n",
        "                    test_loader = get_loader(data=all_corr, samples_list=test_samples,\n",
        "                                             batch_size=batch_size, mode='test', augmentation=False,\n",
        "                                             verbose=verbose, regions=regions_inds)\n",
        "\n",
        "                model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "                model.to(device)\n",
        "                criterion_ae = nn.MSELoss(reduction='sum')\n",
        "                criterion_clf = nn.BCEWithLogitsLoss()\n",
        "                optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                                       {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                                      momentum=0.9)\n",
        "\n",
        "                for epoch in range(1, num_epochs + 1):\n",
        "                    if epoch <= 20:\n",
        "                        train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                    else:\n",
        "                        train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "                res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
        "                #print(\"fold\",kk+1,\":\",test(model, criterion_ae, test_loader, eval_classifier=True))\n",
        "                crossval_res_kol.append(res_mlp)\n",
        "            print(\"Result of repeat \",rp,\":\")\n",
        "            print(np.mean(np.array(crossval_res_kol), axis=0))\n",
        "            all_rp_res.append(np.mean(np.array(crossval_res_kol), axis=0))\n",
        "            finish_rp = time.time()\n",
        "\n",
        "            print(\"Running time:\", finish_rp - start_rp)\n",
        "        print(\"Avergae result of 10 repeats: \", np.mean(np.array(all_rp_res), axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1SmoKq_Q51w",
        "outputId": "70d10ba6-783f-4f1f-c439-6463fae03eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 5-fold cross-validation for center Stanford.\n",
            "p_bernoulli:  None\n",
            "augmentation:  True aug_factor:  2 num_neighbs:  5 lim4sim:  2\n",
            "use_dropout:  False \n",
            "\n",
            "========================\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Result of repeat  0 :\n",
            "[0.475      0.81666667 0.15      ]\n",
            "Running time: 29.545281410217285\n",
            "========================\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Result of repeat  1 :\n",
            "[0.61428571 0.53333333 0.6       ]\n",
            "Running time: 28.08485770225525\n",
            "========================\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Result of repeat  2 :\n",
            "[0.58571429 0.46666667 0.7       ]\n",
            "Running time: 28.676987171173096\n",
            "========================\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Result of repeat  3 :\n",
            "[0.5 0.6 0.4]\n",
            "Running time: 28.15434241294861\n",
            "========================\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Result of repeat  4 :\n",
            "[0.61428571 0.6        0.6       ]\n",
            "Running time: 28.136966228485107\n",
            "========================\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Result of repeat  5 :\n",
            "[0.47142857 0.75       0.25      ]\n",
            "Running time: 27.90436339378357\n",
            "========================\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Result of repeat  6 :\n",
            "[0.58571429 0.4        0.75      ]\n",
            "Running time: 28.206318378448486\n",
            "========================\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Result of repeat  7 :\n",
            "[0.47142857 1.         0.        ]\n",
            "Running time: 28.34106159210205\n",
            "========================\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Result of repeat  8 :\n",
            "[0.55714286 0.63333333 0.45      ]\n",
            "Running time: 28.239425659179688\n",
            "========================\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Classifier input vector shape (with pheno data): torch.Size([8, 4979])\n",
            "Result of repeat  9 :\n",
            "[0.50357143 0.4        0.56666667]\n",
            "Running time: 28.35506796836853\n",
            "Avergae result of 10 repeats:  [0.53785714 0.62       0.44666667]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}