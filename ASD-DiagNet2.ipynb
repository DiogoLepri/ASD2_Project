{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiogoLepri/ASD2_Project/blob/main/ASD-DiagNet2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mVAfnd06ES9",
        "outputId": "bfe068f8-dc2e-421b-92ae-4992757a2090"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ASD2_Project/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnJkylVA6DuC",
        "outputId": "27d488da-0271-4fe1-9e29-bc5633d7c5dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ASD2_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy matplotlib scikit-learn torch pyprind scipy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0I3QvBKj_lM",
        "outputId": "bd6ad648-ad68-4d12-af31-5b698d4b77a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: pyprind in /usr/local/lib/python3.11/dist-packages (2.11.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "IF69HS76je7I"
      },
      "outputs": [],
      "source": [
        "#options: cc200, dosenbach160, aal\n",
        "p_ROI = \"cc200\"\n",
        "p_fold = 10\n",
        "p_center = \"Caltech\"\n",
        "p_mode = \"whole\"\n",
        "p_augmentation = True\n",
        "p_Method = \"ASD-DiagNet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1Oc4GXGje7K",
        "outputId": "3ef0d471-95fd-47f4-be28-c33a8bd37914"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****List of patameters****\n",
            "ROI atlas:  cc200\n",
            "per Center or whole:  whole\n",
            "Method's name:  ASD-DiagNet\n",
            "Augmentation:  True\n"
          ]
        }
      ],
      "source": [
        "parameter_list = [p_ROI,p_fold,p_center,p_mode,p_augmentation,p_Method]\n",
        "print(\"*****List of patameters****\")\n",
        "print(\"ROI atlas: \",p_ROI)\n",
        "print(\"per Center or whole: \",p_mode)\n",
        "if p_mode == 'percenter':\n",
        "    print(\"Center's name: \",p_center)\n",
        "print(\"Method's name: \",p_Method)\n",
        "if p_Method == \"ASD-DiagNet\":\n",
        "    print(\"Augmentation: \",p_augmentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jUGCxT3qje7L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from functools import reduce\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import pyprind\n",
        "import sys\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy import stats\n",
        "from sklearn import tree\n",
        "import functools\n",
        "import numpy.ma as ma # for masked arrays\n",
        "import pyprind\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7-Rr-nGje7L"
      },
      "source": [
        "## Importing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bp8xZtF5je7M"
      },
      "outputs": [],
      "source": [
        "def get_key(filename):\n",
        "    f_split = filename.split('_')\n",
        "    if f_split[3] == 'rois':\n",
        "        key = '_'.join(f_split[0:3])\n",
        "    else:\n",
        "        key = '_'.join(f_split[0:2])\n",
        "    return key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_zvQk6fje7M",
        "outputId": "5698c4aa-e3e0-48b9-a895-4e6a6375bb90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of files in data_main_path: 884\n",
            "Columns in df_labels: Index(['Unnamed: 0.1', 'Unnamed: 0', 'SUB_ID', 'X', 'subject', 'SITE_ID',\n",
            "       'FILE_ID', 'DX_GROUP', 'DSM_IV_TR', 'AGE_AT_SCAN',\n",
            "       ...\n",
            "       'qc_notes_rater_1', 'qc_anat_rater_2', 'qc_anat_notes_rater_2',\n",
            "       'qc_func_rater_2', 'qc_func_notes_rater_2', 'qc_anat_rater_3',\n",
            "       'qc_anat_notes_rater_3', 'qc_func_rater_3', 'qc_func_notes_rater_3',\n",
            "       'SUB_IN_SMP'],\n",
            "      dtype='object', length=106)\n",
            "Number of rows in df_labels: 1112\n",
            "Example entries from phen_dict (FILE_ID: [norm_age]): [('Pitt_0050003', [0.92095]), ('Pitt_0050004', [0.25398603]), ('Pitt_0050005', [-0.41297793]), ('Pitt_0050006', [-0.45777398]), ('Pitt_0050007', [0.09097814])]\n"
          ]
        }
      ],
      "source": [
        "# Path to your time series data\n",
        "data_main_path = f'/content/drive/MyDrive/ASD2_Project/Outputs/cpac/filt_global/rois_{p_ROI}'\n",
        "flist = os.listdir(data_main_path)\n",
        "print(\"Number of files in data_main_path:\", len(flist))\n",
        "\n",
        "# Convert filenames to keys\n",
        "for f in range(len(flist)):\n",
        "    flist[f] = get_key(flist[f])\n",
        "\n",
        "# Load phenotypic CSV\n",
        "df_labels = pd.read_csv('/content/drive/MyDrive/ASD2_Project/Phenotypic_V1_0b_preprocessed1.csv')\n",
        "print(\"Columns in df_labels:\", df_labels.columns)\n",
        "\n",
        "# Convert ASD/Control to 1/0\n",
        "df_labels.DX_GROUP = df_labels.DX_GROUP.map({1: 1, 2: 0})\n",
        "print(\"Number of rows in df_labels:\", len(df_labels))\n",
        "\n",
        "# Build the 'labels' dictionary for your dataset\n",
        "labels = {}\n",
        "for i, row in df_labels.iterrows():\n",
        "    file_id = row['FILE_ID']\n",
        "    y_label = row['DX_GROUP']\n",
        "    if file_id == 'no_filename':\n",
        "        continue\n",
        "    assert(file_id not in labels)\n",
        "    labels[file_id] = y_label\n",
        "\n",
        "# Now, load and normalize the age column (assumes column is named 'AGE_AT_SCAN')\n",
        "# Adjust if your CSV uses a different name for the age column\n",
        "df_labels['AGE_AT_SCAN'] = df_labels['AGE_AT_SCAN'].astype(np.float32)\n",
        "ages = df_labels['AGE_AT_SCAN']\n",
        "ages_norm = (ages - ages.mean()) / ages.std()\n",
        "\n",
        "# Build a phen_dict mapping FILE_ID -> normalized age\n",
        "phen_dict = {}\n",
        "for i, row in df_labels.iterrows():\n",
        "    file_id = row['FILE_ID']\n",
        "    if file_id == 'no_filename':\n",
        "        continue\n",
        "    # Use the same index 'i' to grab the normalized age\n",
        "    phen_dict[file_id] = [ages_norm.iloc[i]]\n",
        "\n",
        "# Quick check\n",
        "print(\"Example entries from phen_dict (FILE_ID: [norm_age]):\", list(phen_dict.items())[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP1zt_HDje7M"
      },
      "source": [
        "### Helper functions for computing correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a7y4oZhgje7M"
      },
      "outputs": [],
      "source": [
        "def get_label(filename):\n",
        "    assert (filename in labels)\n",
        "    return labels[filename]\n",
        "\n",
        "\n",
        "def get_corr_data(filename):\n",
        "    #print(filename)\n",
        "    for file in os.listdir(data_main_path):\n",
        "        if file.startswith(filename):\n",
        "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
        "        m = ma.masked_where(mask == 1, mask)\n",
        "        return ma.masked_where(m, corr).compressed()\n",
        "\n",
        "def get_corr_matrix(filename):\n",
        "    for file in os.listdir(data_main_path):\n",
        "        if file.startswith(filename):\n",
        "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        return corr\n",
        "\n",
        "def confusion(g_turth,predictions):\n",
        "    tn, fp, fn, tp = confusion_matrix(g_turth,predictions).ravel()\n",
        "    accuracy = (tp+tn)/(tp+fp+tn+fn)\n",
        "    sensitivity = (tp)/(tp+fn)\n",
        "    specificty = (tn)/(tn+fp)\n",
        "    return accuracy,sensitivity,specificty\n",
        "\n",
        "def get_regs(samplesnames,regnum):\n",
        "    datas = []\n",
        "    for sn in samplesnames:\n",
        "        datas.append(all_corr[sn][0])\n",
        "    datas = np.array(datas)\n",
        "    avg=[]\n",
        "    for ie in range(datas.shape[1]):\n",
        "        avg.append(np.mean(datas[:,ie]))\n",
        "    avg=np.array(avg)\n",
        "    highs=avg.argsort()[-regnum:][::-1]\n",
        "    lows=avg.argsort()[:regnum][::-1]\n",
        "    regions=np.concatenate((highs,lows),axis=0)\n",
        "    return regions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_df_Lrp8je7M"
      },
      "source": [
        "## Helper fnuctions for computing correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_980i7pjje7N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad298c73-08ce-4cdb-cbe9-d5e424e07d58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corr-computations finished\n",
            "Saving to file finished\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('./correlations_file'+p_ROI+'.pkl'):\n",
        "    pbar=pyprind.ProgBar(len(flist))\n",
        "    all_corr = {}\n",
        "    for f in flist:\n",
        "\n",
        "        lab = get_label(f)\n",
        "        all_corr[f] = (get_corr_data(f), lab)\n",
        "        pbar.update()\n",
        "\n",
        "    print('Corr-computations finished')\n",
        "\n",
        "    pickle.dump(all_corr, open('./correlations_file'+p_ROI+'.pkl', 'wb'))\n",
        "    print('Saving to file finished')\n",
        "\n",
        "else:\n",
        "    all_corr = pickle.load(open('./correlations_file'+p_ROI+'.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vFuNrLsje7N"
      },
      "source": [
        "## Computing eigenvalues and eigenvector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Putbeqefje7N",
        "outputId": "7245af73-d7f6-466e-e627-f3d72bd140df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:01:45\n"
          ]
        }
      ],
      "source": [
        "if p_Method==\"ASD-DiagNet\":\n",
        "    eig_data = {}\n",
        "    pbar = pyprind.ProgBar(len(flist))\n",
        "    for f in flist:\n",
        "        d = get_corr_matrix(f)\n",
        "        eig_vals, eig_vecs = np.linalg.eig(d)\n",
        "\n",
        "        for ev in eig_vecs.T:\n",
        "            np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
        "\n",
        "        sum_eigvals = np.sum(np.abs(eig_vals))\n",
        "        # Make a list of (eigenvalue, eigenvector, norm_eigval) tuples\n",
        "        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i], np.abs(eig_vals[i])/sum_eigvals)\n",
        "                     for i in range(len(eig_vals))]\n",
        "\n",
        "        # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
        "        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        eig_data[f] = {'eigvals':np.array([ep[0] for ep in eig_pairs]),\n",
        "                       'norm-eigvals':np.array([ep[2] for ep in eig_pairs]),\n",
        "                       'eigvecs':[ep[1] for ep in eig_pairs]}\n",
        "        pbar.update()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mUbBcVXje7N"
      },
      "source": [
        "## Calculating Eros similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "y9GN9yITje7N"
      },
      "outputs": [],
      "source": [
        "def norm_weights(sub_flist):\n",
        "    num_dim = len(eig_data[flist[0]]['eigvals'])\n",
        "    norm_weights = np.zeros(shape=num_dim)\n",
        "    for f in sub_flist:\n",
        "        norm_weights += eig_data[f]['norm-eigvals']\n",
        "    return norm_weights\n",
        "\n",
        "def cal_similarity(d1, d2, weights, lim=None):\n",
        "    res = 0.0\n",
        "    if lim is None:\n",
        "        weights_arr = weights.copy()\n",
        "    else:\n",
        "        weights_arr = weights[:lim].copy()\n",
        "        weights_arr /= np.sum(weights_arr)\n",
        "    for i,w in enumerate(weights_arr):\n",
        "        res += w*np.inner(d1[i], d2[i])\n",
        "    return res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmfYEmZije7N"
      },
      "source": [
        "## Defining dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aiJEROcRje7N"
      },
      "outputs": [],
      "source": [
        "class CC200Dataset(Dataset):\n",
        "    def __init__(self, pkl_filename=None, data=None, samples_list=None,\n",
        "                 phenotype_data=None,  # New parameter for phenotypic info (age)\n",
        "                 augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "                 eig_data=None, similarity_fn=None, verbose=False, regs=None):\n",
        "        self.regs = regs\n",
        "        self.phenotype_data = phenotype_data  # store phenotypic info (e.g., age)\n",
        "        if pkl_filename is not None:\n",
        "            if verbose:\n",
        "                print('Loading ..!', end=' ')\n",
        "            self.data = pickle.load(open(pkl_filename, 'rb'))\n",
        "        elif data is not None:\n",
        "            self.data = data.copy()\n",
        "        else:\n",
        "            sys.stderr.write('Either PKL file or data is needed!')\n",
        "            return\n",
        "\n",
        "        # Prepare the sample list\n",
        "        if samples_list is None:\n",
        "            self.flist = [f for f in self.data]\n",
        "        else:\n",
        "            self.flist = [f for f in samples_list]\n",
        "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
        "\n",
        "        current_flist = np.array(self.flist.copy())\n",
        "        current_lab0_flist = current_flist[self.labels == 0]\n",
        "        current_lab1_flist = current_flist[self.labels == 1]\n",
        "\n",
        "        if augmentation:\n",
        "            self.num_data = aug_factor * len(self.flist)\n",
        "            self.neighbors = {}\n",
        "            pbar = pyprind.ProgBar(len(self.flist))\n",
        "            weights = norm_weights(samples_list)  # assuming norm_weights is defined\n",
        "            for f in self.flist:\n",
        "                label = self.data[f][1]\n",
        "                candidates = (set(current_lab0_flist) if label == 0 else set(current_lab1_flist))\n",
        "                candidates.remove(f)\n",
        "                eig_f = eig_data[f]['eigvecs']\n",
        "                sim_list = []\n",
        "                for cand in candidates:\n",
        "                    eig_cand = eig_data[cand]['eigvecs']\n",
        "                    sim = similarity_fn(eig_f, eig_cand, weights)\n",
        "                    sim_list.append((sim, cand))\n",
        "                sim_list.sort(key=lambda x: x[0], reverse=True)\n",
        "                self.neighbors[f] = [item[1] for item in sim_list[:num_neighbs]]\n",
        "        else:\n",
        "            self.num_data = len(self.flist)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Non-augmented samples\n",
        "        if index < len(self.flist):\n",
        "            fname = self.flist[index]\n",
        "            data = self.data[fname][0].copy()  # get_corr_data(fname, mode=cal_mode)\n",
        "            data = data[self.regs].copy()\n",
        "            label = (self.labels[index],)\n",
        "            if self.phenotype_data is not None:\n",
        "                # Retrieve phenotype (age) using the file identifier as key\n",
        "                age_val = self.phenotype_data[fname]\n",
        "                return torch.FloatTensor(data), torch.FloatTensor(age_val), torch.FloatTensor(label)\n",
        "            else:\n",
        "                return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "        else:\n",
        "            # Augmentation branch: mix two samples\n",
        "            f1 = self.flist[index % len(self.flist)]\n",
        "            d1, y1 = self.data[f1][0], self.data[f1][1]\n",
        "            d1 = d1[self.regs]\n",
        "            if len(self.neighbors[f1]) > 0:\n",
        "                f2 = np.random.choice(self.neighbors[f1])\n",
        "            else:\n",
        "                f2 = f1  # fallback to self if no neighbors exist\n",
        "            d2, y2 = self.data[f2][0], self.data[f2][1]\n",
        "            d2 = d2[self.regs]\n",
        "            assert y1 == y2\n",
        "            r = np.random.uniform(low=0, high=1)\n",
        "            data = r * d1 + (1 - r) * d2\n",
        "            label = (y1,)\n",
        "            if self.phenotype_data is not None:\n",
        "                # Use phenotype from the first sample for the augmented data\n",
        "                age_val = self.phenotype_data[f1]\n",
        "                return torch.FloatTensor(data), torch.FloatTensor(age_val), torch.FloatTensor(label)\n",
        "            else:\n",
        "                return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfUMb1Z2je7O"
      },
      "source": [
        "## Definig data loader function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GlGMx4Lrje7O"
      },
      "outputs": [],
      "source": [
        "def get_loader(pkl_filename=None, data=None, samples_list=None,\n",
        "               batch_size=64,\n",
        "               num_workers=1, mode='train',\n",
        "               *, augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "               eig_data=None, similarity_fn=None, verbose=False, regions=None,\n",
        "               phenotype_data=None):  # New parameter for phenotype_data\n",
        "    \"\"\"Build and return data loader.\"\"\"\n",
        "    if mode == 'train':\n",
        "        shuffle = True\n",
        "    else:\n",
        "        shuffle = False\n",
        "        augmentation = False\n",
        "\n",
        "    dataset = CC200Dataset(pkl_filename=pkl_filename, data=data, samples_list=samples_list,\n",
        "                           augmentation=augmentation, aug_factor=aug_factor,\n",
        "                           eig_data=eig_data, similarity_fn=similarity_fn, verbose=verbose, regs=regions,\n",
        "                           phenotype_data=phenotype_data)  # Pass phenotype_data here\n",
        "\n",
        "    data_loader = DataLoader(dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers)\n",
        "\n",
        "    return data_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK8W0nMSje7O"
      },
      "source": [
        "## Defining Autoencoder class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1pd8JMdje7O",
        "outputId": "cf49c735-9a2a-413e-b20a-6fdb6a59e698"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MTAutoEncoder(\n",
              "  (fc_encoder): Linear(in_features=990, out_features=200, bias=True)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=201, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "class MTAutoEncoder(nn.Module):\n",
        "    def __init__(self, num_inputs=990,\n",
        "                 num_latent=200, tied=True,\n",
        "                 num_classes=2, use_dropout=False):\n",
        "        super(MTAutoEncoder, self).__init__()\n",
        "        self.tied = tied\n",
        "        self.num_latent = num_latent\n",
        "\n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "\n",
        "        if not tied:\n",
        "            self.fc_decoder = nn.Linear(num_latent, num_inputs)\n",
        "\n",
        "        # If using dropout, incorporate it before the classifier.\n",
        "        # Note: The classifier now takes num_latent+1 inputs.\n",
        "        if use_dropout:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Dropout(p=0.5),\n",
        "                nn.Linear(self.num_latent + 1, 1)\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(self.num_latent + 1, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, age_data=None, eval_classifier=False):\n",
        "        # Encode the input to get the latent representation\n",
        "        latent = self.fc_encoder(x)\n",
        "        latent = torch.tanh(latent)\n",
        "\n",
        "        if eval_classifier:\n",
        "            if age_data is None:\n",
        "                raise ValueError(\"Classifier mode requires age_data input\")\n",
        "            # Concatenate the latent representation with age_data\n",
        "            # (Ensure age_data is of shape [batch_size, 1])\n",
        "            combined = torch.cat((latent, age_data), dim=1)\n",
        "            x_logit = self.classifier(combined)\n",
        "        else:\n",
        "            x_logit = None\n",
        "\n",
        "        # Reconstruct the input\n",
        "        if self.tied:\n",
        "            rec = F.linear(latent, self.fc_encoder.weight.t())\n",
        "        else:\n",
        "            rec = self.fc_decoder(latent)\n",
        "\n",
        "        return rec, x_logit\n",
        "\n",
        "mtae = MTAutoEncoder()\n",
        "mtae\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLxPmE4kje7O"
      },
      "source": [
        "## Defining training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Glfo2XVQje7O"
      },
      "outputs": [],
      "source": [
        "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n",
        "    model.train()\n",
        "    train_losses = []\n",
        "    for i, (images, age_data, batch_y) in enumerate(train_loader):\n",
        "        # Skip incomplete batches\n",
        "        if len(images) != batch_size:\n",
        "            continue\n",
        "\n",
        "        if p_bernoulli is not None:\n",
        "            if i == 0:\n",
        "                p_tensor = torch.ones_like(images).to(device) * p_bernoulli\n",
        "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
        "\n",
        "        images = images.to(device)\n",
        "        age_data = age_data.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Autoencoder branch\n",
        "        if mode in ['both', 'ae']:\n",
        "            if p_bernoulli is not None:\n",
        "                rec_noisy, _ = model(images * rand_bernoulli, age_data, False)\n",
        "                loss_ae = criterion_ae(rec_noisy, images) / len(images)\n",
        "            else:\n",
        "                rec, _ = model(images, age_data, False)\n",
        "                loss_ae = criterion_ae(rec, images) / len(images)\n",
        "\n",
        "        # Classifier branch\n",
        "        if mode in ['both', 'clf']:\n",
        "            rec_clean, logits = model(images, age_data, True)\n",
        "            loss_clf = criterion_clf(logits, batch_y)\n",
        "\n",
        "        # Combine losses based on mode\n",
        "        if mode == 'both':\n",
        "            loss_total = loss_ae + lam_factor * loss_clf\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), loss_clf.detach().cpu().numpy()])\n",
        "        elif mode == 'ae':\n",
        "            loss_total = loss_ae\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), 0.0])\n",
        "        elif mode == 'clf':\n",
        "            loss_total = loss_clf\n",
        "            train_losses.append([0.0, loss_clf.detach().cpu().numpy()])\n",
        "\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return train_losses\n",
        "\n",
        "\n",
        "\n",
        "def test(model, criterion, test_loader, eval_classifier=False, num_batch=None):\n",
        "    test_loss, n_test, correct = 0.0, 0, 0\n",
        "    all_predss = []\n",
        "    if eval_classifier:\n",
        "        y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for i, (images, age_data, batch_y) in enumerate(test_loader, 1):\n",
        "            if num_batch is not None and i >= num_batch:\n",
        "                continue\n",
        "            images = images.to(device)\n",
        "            age_data = age_data.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "            rec, logits = model(images, age_data, eval_classifier)\n",
        "            test_loss += criterion(rec, images).detach().cpu().numpy()\n",
        "            n_test += len(images)\n",
        "            if eval_classifier:\n",
        "                proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                preds = np.ones_like(proba, dtype=np.int32)\n",
        "                preds[proba < 0.5] = 0\n",
        "                all_predss.extend(preds)\n",
        "                y_arr = batch_y.cpu().numpy().astype(np.int32)\n",
        "                correct += np.sum(preds == y_arr)\n",
        "                y_true.extend(y_arr.tolist())\n",
        "                y_pred.extend(proba.tolist())\n",
        "        if eval_classifier:\n",
        "            mlp_acc, mlp_sens, mlp_spef = confusion(y_true, all_predss)\n",
        "    return mlp_acc, mlp_sens, mlp_spef\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLBvWm1Zje7O",
        "outputId": "3a130058-5746-45e0-e830-01cd9bf9d0c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "bjxpvdXUje7P",
        "outputId": "23dd5645-633f-4913-a016-9d4f068449e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num_corr:   19900\n",
            "4975\n",
            "p_bernoulli:  None\n",
            "augmentaiton:  True aug_factor:  2 num_neighbs:  5 lim4sim:  2\n",
            "use_dropout:  False \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-367606b3a874>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mnum_inpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregions_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mn_lat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_inpp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             train_loader=get_loader(data=all_corr, samples_list=train_samples,\n\u001b[0m\u001b[1;32m     48\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                                     \u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-4210f6dc1ea2>\u001b[0m in \u001b[0;36mget_loader\u001b[0;34m(pkl_filename, data, samples_list, batch_size, num_workers, mode, augmentation, aug_factor, num_neighbs, eig_data, similarity_fn, verbose, regions, phenotype_data)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0maugmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     dataset = CC200Dataset(pkl_filename=pkl_filename, data=data, samples_list=samples_list,\n\u001b[0m\u001b[1;32m     15\u001b[0m                            \u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                            \u001b[0meig_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meig_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msimilarity_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-13ff3b70ff94>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pkl_filename, data, samples_list, phenotype_data, augmentation, aug_factor, num_neighbs, eig_data, similarity_fn, verbose, regs)\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mcand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0meig_cand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meig_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcand\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eigvecs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                     \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meig_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meig_cand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                     \u001b[0msim_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0msim_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-75263562a837>\u001b[0m in \u001b[0;36mcal_similarity\u001b[0;34m(d1, d2, weights, lim)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mweights_arr\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "if p_Method == \"ASD-DiagNet\" and p_mode == \"whole\":\n",
        "\n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "    print(\"num_corr:  \",num_corr)\n",
        "\n",
        "    start =time.time()\n",
        "    batch_size = 8\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "    num_epochs = 25\n",
        "\n",
        "    p_bernoulli = None\n",
        "    augmentation = p_augmentation\n",
        "    use_dropout = False\n",
        "\n",
        "    aug_factor = 2\n",
        "    num_neighbs = 5\n",
        "    lim4sim = 2\n",
        "    n_lat = int(num_corr/4)\n",
        "    print(n_lat)\n",
        "    start= time.time()\n",
        "\n",
        "    print('p_bernoulli: ', p_bernoulli)\n",
        "    print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor,\n",
        "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "    print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "\n",
        "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "    crossval_res_kol=[]\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "    flist = np.array(flist)\n",
        "    kk=0\n",
        "    for rp in range(10):\n",
        "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "        np.random.shuffle(flist)\n",
        "        y_arr = np.array([get_label(f) for f in flist])\n",
        "        for kk,(train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "            train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "\n",
        "            regions_inds = get_regs(train_samples,int(num_corr/4))\n",
        "\n",
        "            num_inpp = len(regions_inds)\n",
        "            n_lat = int(num_inpp/2)\n",
        "            train_loader=get_loader(data=all_corr, samples_list=train_samples,\n",
        "                                    batch_size=batch_size, mode='train',\n",
        "                                    augmentation=augmentation, aug_factor=aug_factor,\n",
        "                                    num_neighbs=num_neighbs, eig_data=eig_data, similarity_fn=sim_function,\n",
        "                                    verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            test_loader=get_loader(data=all_corr, samples_list=test_samples,\n",
        "                                   batch_size=batch_size, mode='test', augmentation=False,\n",
        "                                   verbose=verbose,regions=regions_inds)\n",
        "\n",
        "            model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "            model.to(device)\n",
        "            criterion_ae = nn.MSELoss(reduction='sum')\n",
        "            criterion_clf = nn.BCEWithLogitsLoss()\n",
        "            optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                                   {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                                  momentum=0.9)\n",
        "\n",
        "            for epoch in range(1, num_epochs+1):\n",
        "                if epoch <= 20:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                else:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "\n",
        "            res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
        "            print(test(model, criterion_ae, test_loader, eval_classifier=True))\n",
        "            crossval_res_kol.append(res_mlp)\n",
        "        print(\"averages:\")\n",
        "        print(np.mean(np.array(crossval_res_kol),axis = 0))\n",
        "        finish= time.time()\n",
        "\n",
        "        print(finish-start)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p_ROI = \"cc200\"\n",
        "p_fold = 5  # Use 5-fold for intra-site evaluation\n",
        "p_center = \"KKI\"\n",
        "p_mode = \"percenter\"  # Change to \"percenter\" for intra-site evaluation\n",
        "p_augmentation = True\n",
        "p_Method = \"ASD-DiagNet\""
      ],
      "metadata": {
        "id": "lHRl9EzZWivb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1oKImd8Eje7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56b399e-ffac-4cbe-c534-759710ab0757"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 5-fold cross-validation for center KKI.\n",
            "Running repeat 1 for center KKI...\n",
            "Result of repeat 1 for center KKI: [0.69285714 0.         1.        ]\n",
            "Running repeat 2 for center KKI...\n",
            "Result of repeat 2 for center KKI: [0.69285714 0.         1.        ]\n",
            "Running repeat 3 for center KKI...\n",
            "Result of repeat 3 for center KKI: [0.69285714 0.         1.        ]\n",
            "Running repeat 4 for center KKI...\n",
            "Result of repeat 4 for center KKI: [0.69285714 0.         1.        ]\n",
            "Running repeat 5 for center KKI...\n",
            "Result of repeat 5 for center KKI: [0.69285714 0.         1.        ]\n",
            "Running repeat 6 for center KKI...\n",
            "Result of repeat 6 for center KKI: [0.69285714 0.         1.        ]\n",
            "Running repeat 7 for center KKI...\n",
            "Result of repeat 7 for center KKI: [0.69285714 0.         1.        ]\n",
            "Running repeat 8 for center KKI...\n",
            "Result of repeat 8 for center KKI: [0.69285714 0.         1.        ]\n",
            "Running repeat 9 for center KKI...\n",
            "Result of repeat 9 for center KKI: [0.69285714 0.         1.        ]\n",
            "Running repeat 10 for center KKI...\n",
            "Result of repeat 10 for center KKI: [0.69285714 0.         1.        ]\n",
            "Average result for 10 repeats for center KKI: [0.69285714 0.         1.        ]\n",
            "Total running time for center KKI: 194.87 seconds\n"
          ]
        }
      ],
      "source": [
        "if p_Method == \"ASD-DiagNet\" and p_mode == \"percenter\":\n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "\n",
        "    flist = os.listdir(data_main_path)\n",
        "    flist = [get_key(f) for f in flist]\n",
        "\n",
        "    centers_dict = {}\n",
        "    for f in flist:\n",
        "        key = f.split('_')[0]\n",
        "        centers_dict.setdefault(key, []).append(f)\n",
        "\n",
        "    flist = np.array(centers_dict[p_center])\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "\n",
        "    # Determine the number of splits dynamically\n",
        "    unique_labels, counts = np.unique(y_arr, return_counts=True)\n",
        "    new_n_splits = min(p_fold, counts.min())\n",
        "\n",
        "    if new_n_splits < 2:\n",
        "        print(f\"Skipping center {p_center} due to insufficient samples in one class.\")\n",
        "    else:\n",
        "        print(f\"Using {new_n_splits}-fold cross-validation for center {p_center}.\")\n",
        "\n",
        "        start = time.time()\n",
        "        batch_size = 8\n",
        "        learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "        num_epochs = 25\n",
        "        p_bernoulli = None\n",
        "        augmentation = p_augmentation\n",
        "        use_dropout = False\n",
        "        aug_factor = 2\n",
        "        num_neighbs = 5\n",
        "        lim4sim = 2\n",
        "        n_lat = int(num_corr / 4)\n",
        "\n",
        "        sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "        all_rp_res = []\n",
        "\n",
        "        for rp in range(10):\n",
        "            print(f\"Running repeat {rp + 1} for center {p_center}...\")\n",
        "            crossval_res_kol = []\n",
        "            kf = StratifiedKFold(n_splits=new_n_splits)\n",
        "            for kk, (train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "                train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "                verbose = (kk == 0)\n",
        "                regions_inds = get_regs(train_samples, int(num_corr / 4))\n",
        "                num_inpp = len(regions_inds)\n",
        "                n_lat = int(num_inpp / 2)\n",
        "\n",
        "                train_loader = get_loader(data=all_corr, samples_list=train_samples,\n",
        "                                          batch_size=batch_size, mode='train',\n",
        "                                          augmentation=augmentation, aug_factor=aug_factor,\n",
        "                                          num_neighbs=num_neighbs, eig_data=eig_data,\n",
        "                                          similarity_fn=sim_function, verbose=verbose, regions=regions_inds,\n",
        "                                          phenotype_data=phen_dict)\n",
        "\n",
        "                test_loader = get_loader(data=all_corr, samples_list=test_samples,\n",
        "                                         batch_size=batch_size, mode='test', augmentation=False,\n",
        "                                         verbose=verbose, regions=regions_inds,\n",
        "                                         phenotype_data=phen_dict)\n",
        "\n",
        "                model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "                model.to(device)\n",
        "                criterion_ae = nn.MSELoss(reduction='sum')\n",
        "                criterion_clf = nn.BCEWithLogitsLoss()\n",
        "                optimizer = optim.SGD([{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                                       {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                                      momentum=0.9)\n",
        "\n",
        "                for epoch in range(1, num_epochs + 1):\n",
        "                    if epoch <= 20:\n",
        "                        train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                    else:\n",
        "                        train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "                res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
        "                crossval_res_kol.append(res_mlp)\n",
        "\n",
        "            print(f\"Result of repeat {rp + 1} for center {p_center}: {np.mean(np.array(crossval_res_kol), axis=0)}\")\n",
        "            all_rp_res.append(np.mean(np.array(crossval_res_kol), axis=0))\n",
        "\n",
        "        print(f\"Average result for 10 repeats for center {p_center}: {np.mean(np.array(all_rp_res), axis=0)}\")\n",
        "        finish = time.time()\n",
        "        print(f\"Total running time for center {p_center}: {finish - start:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result Paper - Result Mine:\n",
        "\n",
        "Accuracy%\n",
        "\n",
        "  O%   -   1%     -     2%\n",
        "\n",
        "Caltech: 52.8% - 60.5%\n",
        "\n",
        "CMU: 68.5% - 51.6%\n",
        "\n",
        "KKI: 69.5% - 69.3%\n",
        "\n",
        "Leuven: 61.3% - 54.1%\n",
        "\n",
        "Maxmun: 48.6% - 63.9%\n",
        "\n",
        "NYU: 68.0% - 67.9%\n",
        "\n",
        "OHSU: 82% - 73.3%\n",
        "\n",
        "Olin: 65.1% - 68.8%\n",
        "\n",
        "Pitt: 67.8% - 63.3%\n",
        "\n",
        "SBL: 51.6% - 43.6%\n",
        "\n",
        "SDSU: 63.0% - 64.0%\n",
        "\n",
        "Stanford: 64.2% - 64.9%\n",
        "\n",
        "Trinity: 54.1% - 53.1%\n",
        "\n",
        "UCLA: 73.2% - 67.1%\n",
        "\n",
        "USM: 68.2% - 68.2%\n",
        "\n",
        "UM: 63.8% - 65.7%\n",
        "\n",
        "Yale: 63.6% - 64.1%"
      ],
      "metadata": {
        "id": "GJN6BZtfXQF2"
      }
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}