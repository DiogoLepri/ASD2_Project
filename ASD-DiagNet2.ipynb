{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiogoLepri/ASD2_Project/blob/main/ASD_DiagNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mVAfnd06ES9",
        "outputId": "210fdbe2-5106-4d06-e228-51adfc3b8638"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/ASD2_Project/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnJkylVA6DuC",
        "outputId": "36e5e15b-f4f3-4ec1-e382-d9c73192d4ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ASD2_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy matplotlib scikit-learn torch pyprind scipy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0I3QvBKj_lM",
        "outputId": "b8759409-8f14-42b0-e85a-8b0587027831"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "IF69HS76je7I"
      },
      "outputs": [],
      "source": [
        "#options: cc200, dosenbach160, aal\n",
        "p_ROI = \"cc200\"\n",
        "p_fold = 10\n",
        "p_center = \"Stanford\"\n",
        "p_mode = \"whole\"\n",
        "p_augmentation = True\n",
        "p_Method = \"ASD-DiagNet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1Oc4GXGje7K",
        "outputId": "4555a521-ea99-405d-eaf8-9e36b2c1a15a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****List of patameters****\n",
            "ROI atlas:  cc200\n",
            "per Center or whole:  whole\n",
            "Method's name:  ASD-DiagNet\n",
            "Augmentation:  True\n"
          ]
        }
      ],
      "source": [
        "parameter_list = [p_ROI,p_fold,p_center,p_mode,p_augmentation,p_Method]\n",
        "print(\"*****List of patameters****\")\n",
        "print(\"ROI atlas: \",p_ROI)\n",
        "print(\"per Center or whole: \",p_mode)\n",
        "if p_mode == 'percenter':\n",
        "    print(\"Center's name: \",p_center)\n",
        "print(\"Method's name: \",p_Method)\n",
        "if p_Method == \"ASD-DiagNet\":\n",
        "    print(\"Augmentation: \",p_augmentation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jUGCxT3qje7L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from functools import reduce\n",
        "from sklearn.impute import SimpleImputer\n",
        "import time\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import pyprind\n",
        "import sys\n",
        "import pickle\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from scipy import stats\n",
        "from sklearn import tree\n",
        "import functools\n",
        "import numpy.ma as ma # for masked arrays\n",
        "import pyprind\n",
        "import random\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7-Rr-nGje7L"
      },
      "source": [
        "## Importing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bp8xZtF5je7M"
      },
      "outputs": [],
      "source": [
        "def get_key(filename):\n",
        "    f_split = filename.split('_')\n",
        "    if f_split[3] == 'rois':\n",
        "        key = '_'.join(f_split[0:3])\n",
        "    else:\n",
        "        key = '_'.join(f_split[0:2])\n",
        "    return key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_zvQk6fje7M",
        "outputId": "38d3e16d-31ce-4039-c0a0-42e292df70f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "884\n",
            "1112\n"
          ]
        }
      ],
      "source": [
        "# Define the path to the directory containing time series data.\n",
        "# The f-string here dynamically inserts the value of p_ROI into the folder name.\n",
        "data_main_path = f'/content/drive/MyDrive/ASD2_Project/Outputs/cpac/filt_global/rois_{p_ROI}'\n",
        "\n",
        "# List all files in the directory (each file likely corresponds to a subject's data).\n",
        "flist = os.listdir(data_main_path)\n",
        "print(len(flist))  # Print how many files were found.\n",
        "\n",
        "\n",
        "for f in range(len(flist)):\n",
        "    flist[f] = get_key(flist[f])\n",
        "\n",
        "\n",
        "df_labels = pd.read_csv('/content/drive/MyDrive/ASD2_Project/Phenotypic_V1_0b_preprocessed1.csv')#path\n",
        "\n",
        "# Map the DX_GROUP column so that a value of 1 stays 1 (ASD) and a value of 2 becomes 0 (control)\n",
        "df_labels.DX_GROUP = df_labels.DX_GROUP.map({1: 1, 2:0})\n",
        "print(len(df_labels))\n",
        "\n",
        "labels = {}  # Initialize an empty dictionary to store labels.\n",
        "\n",
        "# Iterate over each row in the DataFrame.\n",
        "for row in df_labels.iterrows():\n",
        "    file_id = row[1]['FILE_ID']  # Get the file identifier for the subject.\n",
        "    y_label = row[1]['DX_GROUP']  # Get the diagnosis label (1 or 0).\n",
        "\n",
        "    # Skip rows where there is no valid filename.\n",
        "    if file_id == 'no_filename':\n",
        "        continue\n",
        "\n",
        "    # Ensure that each file_id is unique. If a duplicate is found, the program will stop here.\n",
        "    assert(file_id not in labels)\n",
        "\n",
        "    # Add the file_id and its corresponding label to the dictionary.\n",
        "    labels[file_id] = y_label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP1zt_HDje7M"
      },
      "source": [
        "### Helper functions for computing correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a7y4oZhgje7M"
      },
      "outputs": [],
      "source": [
        "def get_label(filename):\n",
        "    # Check that the filename exists in the labels dictionary.\n",
        "    # If it doesn't, the program will raise an AssertionError(if the expression return false.)\n",
        "    assert (filename in labels)\n",
        "\n",
        "    # Return the label associated with this filename from the labels dictionary.\n",
        "    return labels[filename]\n",
        "\n",
        "\n",
        "\n",
        "def get_corr_data(filename):\n",
        "    # Look through all files in the data directory\n",
        "    for file in os.listdir(data_main_path):\n",
        "        # If a file's name starts with the given filename string...\n",
        "        if file.startswith(filename):\n",
        "            # Read the file into a pandas DataFrame.\n",
        "            # Files are assumed to be tab-separated values.\n",
        "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "\n",
        "    # Temporarily ignore warnings about invalid numerical operations (e.g., division by zero)\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        # Compute the correlation matrix for the transposed DataFrame.\n",
        "        # Transposing (df.T) is done because np.corrcoef expects rows as variables.\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        # Create a lower-triangular matrix mask.\n",
        "        # np.tri creates a lower-triangular matrix of ones (True) below the main diagonal,\n",
        "        # but here k=-1 means the main diagonal is not included.\n",
        "        # np.invert flips True to False and vice versa, so 'mask' becomes True for\n",
        "        # the diagonal and upper-triangle, and False for the lower triangle.\n",
        "        mask = np.invert(np.tri(corr.shape[0], k=-1, dtype=bool))\n",
        "        # Create a masked array 'm' that masks out values where the mask is True.\n",
        "        m = ma.masked_where(mask == 1, mask)\n",
        "        # Apply the mask 'm' to the correlation matrix and then compress it.\n",
        "        # .compressed() returns a 1D array of only the unmasked (i.e., selected) values.\n",
        "        return ma.masked_where(m, corr).compressed()\n",
        "\n",
        "\n",
        "def get_corr_matrix(filename):\n",
        "    # Look through all files in the data directory.\n",
        "    for file in os.listdir(data_main_path):\n",
        "        # If a file's name starts with the given filename...\n",
        "        if file.startswith(filename):\n",
        "            # Read the file into a DataFrame (assuming tab-separated values).\n",
        "            df = pd.read_csv(os.path.join(data_main_path, file), sep='\\t')\n",
        "    # Again, ignore warnings about invalid numerical operations.\n",
        "    with np.errstate(invalid=\"ignore\"):\n",
        "        # Compute the correlation matrix for the transposed DataFrame.\n",
        "        corr = np.nan_to_num(np.corrcoef(df.T))\n",
        "        # Return the complete correlation matrix (as a 2D array).\n",
        "        return corr\n",
        "\n",
        "def confusion(g_turth, predictions):\n",
        "    # Compute the confusion matrix from the ground truth and predicted labels.\n",
        "    # The confusion_matrix function returns a 2x2 matrix for binary classification.\n",
        "    # We then \"ravel\" (flatten) it into four numbers:\n",
        "    # tn: true negatives, fp: false positives, fn: false negatives, tp: true positives.\n",
        "    tn, fp, fn, tp = confusion_matrix(g_turth, predictions).ravel()\n",
        "\n",
        "    # Calculate the overall accuracy: (correct predictions) / (total predictions)\n",
        "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
        "\n",
        "    # Calculate the sensitivity (or recall): the proportion of actual positives correctly identified.\n",
        "    sensitivity = tp / (tp + fn)\n",
        "\n",
        "    # Calculate the specificity: the proportion of actual negatives correctly identified.\n",
        "    specificty = tn / (tn + fp)\n",
        "\n",
        "    # Return the computed accuracy, sensitivity, and specificity.\n",
        "    return accuracy, sensitivity, specificty\n",
        "\n",
        "def get_regs(samplesnames, regnum):\n",
        "    datas = []\n",
        "    for sn in samplesnames:\n",
        "        # For each sample name in samplesnames, extract the correlation data.\n",
        "        # Here, all_corr is assumed to be a pre-defined dictionary where each key is a sample name.\n",
        "        # The value all_corr[sn] is expected to be an array or tuple, and we take its first element [0].\n",
        "        datas.append(all_corr[sn][0])\n",
        "\n",
        "    # Convert the list of arrays to a NumPy array.\n",
        "    datas = np.array(datas)\n",
        "\n",
        "    # Calculate the average correlation value for each region/column across all samples.\n",
        "    avg = []\n",
        "    # datas.shape[1] represents the number of regions (or the length of the correlation vector for each sample).\n",
        "    for ie in range(datas.shape[1]):\n",
        "        # For each region (column), compute the mean value across all samples.\n",
        "        avg.append(np.mean(datas[:, ie]))\n",
        "    avg = np.array(avg)\n",
        "\n",
        "    # Identify the indices of the 'regnum' regions with the highest average values.\n",
        "    # argsort returns indices that would sort the array.\n",
        "    # We take the last 'regnum' indices (which correspond to the highest values) and then reverse their order.\n",
        "    highs = avg.argsort()[-regnum:][::-1]\n",
        "\n",
        "    # Similarly, identify the indices of the 'regnum' regions with the lowest average values.\n",
        "    lows = avg.argsort()[:regnum][::-1]\n",
        "\n",
        "    # Concatenate the high and low region indices into one array.\n",
        "    regions = np.concatenate((highs, lows), axis=0)\n",
        "\n",
        "    return regions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_df_Lrp8je7M"
      },
      "source": [
        "## Helper fnuctions for computing correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_980i7pjje7N"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('./correlations_file'+p_ROI+'.pkl'):\n",
        "    # If the correlations file for this specific p_ROI doesn't exist, we will compute the correlations.\n",
        "\n",
        "    # Create a progress bar that will show progress for the number of files in flist.\n",
        "    pbar = pyprind.ProgBar(len(flist))\n",
        "\n",
        "    # Initialize an empty dictionary to store correlation data and corresponding labels for each file.\n",
        "    all_corr = {}\n",
        "\n",
        "    # Loop over each file name in flist.\n",
        "    for f in flist:\n",
        "        # Get the label (e.g., diagnosis) for this file.\n",
        "        lab = get_label(f)\n",
        "        # Compute the correlation data for this file and store it together with the label in the dictionary.\n",
        "        all_corr[f] = (get_corr_data(f), lab)\n",
        "        # Update the progress bar to show that one file has been processed.\n",
        "        pbar.update()\n",
        "\n",
        "    print('Corr-computations finished')\n",
        "\n",
        "    # Save the computed all_corr dictionary to a pickle file.\n",
        "    # The file name is built dynamically by concatenating './correlations_file', the value of p_ROI, and '.pkl'.\n",
        "    pickle.dump(all_corr, open('./correlations_file'+p_ROI+'.pkl', 'wb'))\n",
        "    print('Saving to file finished')\n",
        "\n",
        "else:\n",
        "    # If the correlations file already exists, simply load the precomputed data from the file.\n",
        "    all_corr = pickle.load(open('./correlations_file'+p_ROI+'.pkl', 'rb'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vFuNrLsje7N"
      },
      "source": [
        "## Computing eigenvalues and eigenvector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Putbeqefje7N",
        "outputId": "cc87762f-be8a-43ac-b7a4-046a10cf432a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:03:32\n"
          ]
        }
      ],
      "source": [
        "if p_Method == \"ASD-DiagNet\":\n",
        "    # Initialize an empty dictionary to hold eigenvalue/eigenvector data for each file.\n",
        "    eig_data = {}\n",
        "\n",
        "    # Initialize a progress bar that tracks progress across all files.\n",
        "    pbar = pyprind.ProgBar(len(flist))\n",
        "\n",
        "    # Loop through each file in the list.\n",
        "    for f in flist:\n",
        "        # Compute the full correlation matrix for the current file.\n",
        "        d = get_corr_matrix(f)\n",
        "\n",
        "        # Calculate eigenvalues and eigenvectors of the correlation matrix.\n",
        "        # np.linalg.eig returns a tuple: (eigenvalues, eigenvectors)\n",
        "        eig_vals, eig_vecs = np.linalg.eig(d)\n",
        "\n",
        "        # For each eigenvector, verify that it has unit norm.\n",
        "        # We use the transpose of eig_vecs (i.e., each column is an eigenvector) for iteration.\n",
        "        for ev in eig_vecs.T:\n",
        "            np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
        "\n",
        "        # Compute the sum of the absolute values of the eigenvalues.\n",
        "        # This is used to calculate a normalized eigenvalue (a measure of contribution).\n",
        "        sum_eigvals = np.sum(np.abs(eig_vals))\n",
        "\n",
        "        # Create a list of tuples, where each tuple contains:\n",
        "        # (absolute eigenvalue, corresponding eigenvector, normalized eigenvalue)\n",
        "        eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:, i], np.abs(eig_vals[i]) / sum_eigvals)\n",
        "                     for i in range(len(eig_vals))]\n",
        "\n",
        "        # Sort these tuples in descending order based on the eigenvalue.\n",
        "        # This means the eigenpair with the highest eigenvalue comes first.\n",
        "        eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Store the sorted eigenpairs in the dictionary using the file name as the key.\n",
        "        eig_data[f] = {\n",
        "            'eigvals': np.array([ep[0] for ep in eig_pairs]),       # Sorted eigenvalues\n",
        "            'norm-eigvals': np.array([ep[2] for ep in eig_pairs]),    # Normalized eigenvalues\n",
        "            'eigvecs': [ep[1] for ep in eig_pairs]                    # Corresponding eigenvectors\n",
        "        }\n",
        "\n",
        "        # Update the progress bar after processing each file.\n",
        "        pbar.update()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mUbBcVXje7N"
      },
      "source": [
        "## Calculating Eros similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "y9GN9yITje7N"
      },
      "outputs": [],
      "source": [
        "def norm_weights(sub_flist):\n",
        "    # Get the number of eigenvalues (dimensions) from one sample.\n",
        "    # Here, we use the first file in flist (assumed to be defined globally) to get this value.\n",
        "    num_dim = len(eig_data[flist[0]]['eigvals'])\n",
        "\n",
        "    # Initialize an array of zeros with a length equal to the number of eigenvalues.\n",
        "    norm_weights = np.zeros(shape=num_dim)\n",
        "\n",
        "    # Loop over each file/sample in the provided sublist.\n",
        "    for f in sub_flist:\n",
        "        # For each file, add its normalized eigenvalues to our running sum.\n",
        "        norm_weights += eig_data[f]['norm-eigvals']\n",
        "\n",
        "    # Return the aggregated normalized eigenvalues.\n",
        "    return norm_weights\n",
        "\n",
        "\n",
        "def cal_similarity(d1, d2, weights, lim=None):\n",
        "    res = 0.0\n",
        "    if lim is None:\n",
        "        # If no limit is provided, make a copy of the full weights array.\n",
        "        weights_arr = weights.copy()\n",
        "    else:\n",
        "        # If a limit is provided, take only the first 'lim' elements.\n",
        "        weights_arr = weights[:lim].copy()\n",
        "        # Normalize these weights so that they sum to 1.\n",
        "        weights_arr /= np.sum(weights_arr)\n",
        "\n",
        "    # Loop through each weight in the (possibly limited) weights array.\n",
        "    for i, w in enumerate(weights_arr):\n",
        "        # For each dimension i, compute the inner product of d1[i] and d2[i].\n",
        "        # Then multiply by the weight 'w' and add it to the result.\n",
        "        res += w * np.inner(d1[i], d2[i])\n",
        "\n",
        "    # Return the final weighted similarity.\n",
        "    return ress"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmfYEmZije7N"
      },
      "source": [
        "## Defining dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aiJEROcRje7N"
      },
      "outputs": [],
      "source": [
        "class CC200Dataset(Dataset):\n",
        "    def __init__(self, pkl_filename=None, data=None, samples_list=None,\n",
        "                 augmentation=False, aug_factor=1, num_neighbs=5,\n",
        "                 eig_data=None, similarity_fn=None, verbose=False, regs=None):\n",
        "        # 'regs' is a parameter that selects specific regions (indices) to be used from the data.\n",
        "        self.regs = regs\n",
        "\n",
        "        # Loading the data:\n",
        "        # If a pickle file is provided, load the precomputed data.\n",
        "        if pkl_filename is not None:\n",
        "            if verbose:\n",
        "                print('Loading ..!', end=' ')\n",
        "            self.data = pickle.load(open(pkl_filename, 'rb'))\n",
        "        # Otherwise, if raw data is directly provided, copy it.\n",
        "        elif data is not None:\n",
        "            self.data = data.copy()\n",
        "        else:\n",
        "            sys.stderr.write('Either PKL file or data is needed!')\n",
        "            return\n",
        "\n",
        "        # Determine the list of sample identifiers (filenames or keys).\n",
        "        # If no specific samples_list is provided, use all keys from self.data.\n",
        "        if samples_list is None:\n",
        "            self.flist = [f for f in self.data]\n",
        "        else:\n",
        "            self.flist = [f for f in samples_list]\n",
        "\n",
        "        # Extract labels for each sample.\n",
        "        # It is assumed that self.data[f] is a tuple where index 1 holds the label.\n",
        "        self.labels = np.array([self.data[f][1] for f in self.flist])\n",
        "\n",
        "        # Create arrays for the full list and for each label separately.\n",
        "        current_flist = np.array(self.flist.copy())\n",
        "        current_lab0_flist = current_flist[self.labels == 0]\n",
        "        current_lab1_flist = current_flist[self.labels == 1]\n",
        "        # Optionally, you might print out the counts of each label for verification.\n",
        "\n",
        "        # Data augmentation block:\n",
        "        # If augmentation is True, we will create additional (augmented) samples.\n",
        "        if augmentation:\n",
        "            # Total number of samples will be the original count multiplied by aug_factor.\n",
        "            self.num_data = aug_factor * len(self.flist)\n",
        "            self.neighbors = {}  # This dictionary will store \"neighbors\" for each sample.\n",
        "            pbar = pyprind.ProgBar(len(self.flist))\n",
        "\n",
        "            # Compute aggregated normalized weights using the provided samples_list.\n",
        "            # These weights could be used to measure similarity between eigenvector sets.\n",
        "            weights = norm_weights(samples_list)  # norm_weights() should return an array of weights.\n",
        "\n",
        "            # For every sample, find similar neighbors (only within the same class).\n",
        "            for f in self.flist:\n",
        "                label = self.data[f][1]\n",
        "                # Choose candidate neighbors from the same class.\n",
        "                candidates = set(current_lab0_flist) if label == 0 else set(current_lab1_flist)\n",
        "                # Remove the current sample from its own candidate list.\n",
        "                candidates.remove(f)\n",
        "\n",
        "                # Get the eigenvector data for the current sample.\n",
        "                eig_f = eig_data[f]['eigvecs']\n",
        "                sim_list = []  # This will hold tuples of (similarity, candidate)\n",
        "\n",
        "                # Compute similarity with each candidate using the provided similarity_fn.\n",
        "                for cand in candidates:\n",
        "                    eig_cand = eig_data[cand]['eigvecs']\n",
        "                    sim = similarity_fn(eig_f, eig_cand, weights)\n",
        "                    sim_list.append((sim, cand))\n",
        "\n",
        "                # Sort the candidates by similarity score (highest first).\n",
        "                sim_list.sort(key=lambda x: x[0], reverse=True)\n",
        "                # Save the top 'num_neighbs' similar candidates as neighbors for sample f.\n",
        "                self.neighbors[f] = [item[1] for item in sim_list[:num_neighbs]]\n",
        "                pbar.update()\n",
        "        else:\n",
        "            # If no augmentation is requested, the number of data points equals the number of samples.\n",
        "            self.num_data = len(self.flist)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # The __getitem__ method returns one data sample (and its label).\n",
        "        # It handles both the original and augmented samples.\n",
        "\n",
        "        if index < len(self.flist):\n",
        "            # For indices within the original list, simply retrieve the data.\n",
        "            fname = self.flist[index]\n",
        "            # Retrieve the correlation data (stored at index 0 of the tuple).\n",
        "            data = self.data[fname][0].copy()\n",
        "            # Select only the regions specified by self.regs.\n",
        "            data = data[self.regs].copy()\n",
        "            # Get the label corresponding to this sample.\n",
        "            label = (self.labels[index],)\n",
        "            # Convert to torch.FloatTensor and return.\n",
        "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "        else:\n",
        "            # For indices beyond the original list, perform data augmentation.\n",
        "            # Use modulo to wrap around to an original sample.\n",
        "            f1 = self.flist[index % len(self.flist)]\n",
        "            d1, y1 = self.data[f1][0], self.data[f1][1]\n",
        "            # Subset the data by the regions of interest.\n",
        "            d1 = d1[self.regs]\n",
        "            # Randomly choose one of the precomputed neighbors for f1.\n",
        "            f2 = np.random.choice(self.neighbors[f1])\n",
        "            d2, y2 = self.data[f2][0], self.data[f2][1]\n",
        "            d2 = d2[self.regs]\n",
        "            # Ensure that both samples have the same label.\n",
        "            assert y1 == y2\n",
        "            # Generate a random mixing coefficient.\n",
        "            r = np.random.uniform(low=0, high=1)\n",
        "            # The new data is a convex combination of the two neighbors.\n",
        "            data = r * d1 + (1 - r) * d2\n",
        "            label = (y1,)\n",
        "            # Return the augmented sample as torch.FloatTensor.\n",
        "            return torch.FloatTensor(data), torch.FloatTensor(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples, which might be larger than the original\n",
        "        # number if augmentation is enabled.\n",
        "        return self.num_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfUMb1Z2je7O"
      },
      "source": [
        "## Definig data loader function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GlGMx4Lrje7O"
      },
      "outputs": [],
      "source": [
        "class MTAutoEncoder(nn.Module):\n",
        "    def __init__(self, num_inputs=990,\n",
        "                 num_latent=200, tied=True,\n",
        "                 num_classes=2, use_dropout=False):\n",
        "        super(MTAutoEncoder, self).__init__()\n",
        "        # Store whether the decoder weights are tied to the encoder weights.\n",
        "        self.tied = tied\n",
        "        self.num_latent = num_latent\n",
        "\n",
        "        # Define the encoder layer: transforms input of size 990 to a latent representation of size 200.\n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "\n",
        "        # If we are NOT tying weights, we need a separate decoder layer.\n",
        "        if not tied:\n",
        "            self.fc_decoder = nn.Linear(num_latent, num_inputs)\n",
        "\n",
        "        # NOTE: The encoder layer is defined twice in the code.\n",
        "        # The second definition overwrites the first, so the first one could be removed.\n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "\n",
        "        # Define the classifier head.\n",
        "        # If use_dropout is True, add a dropout layer before the classification.\n",
        "        if use_dropout:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Dropout(p=0.5),  # Drop 50% of the neurons randomly during training for regularization.\n",
        "                nn.Linear(self.num_latent, 1),  # Linear layer mapping latent features to 1 output (e.g., for binary classification).\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(self.num_latent, 1),\n",
        "            )\n",
        "\n",
        "    def forward(self, x, eval_classifier=False):\n",
        "        # Pass input through the encoder to get the latent representation.\n",
        "        x = self.fc_encoder(x)\n",
        "        # Apply the tanh activation function to introduce non-linearity.\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # If we are evaluating the classifier (e.g., during a classification task),\n",
        "        # pass the latent features through the classifier head.\n",
        "        if eval_classifier:\n",
        "            x_logit = self.classifier(x)\n",
        "        else:\n",
        "            x_logit = None\n",
        "\n",
        "        # For the reconstruction part:\n",
        "        if self.tied:\n",
        "            # Tied weights: Use the transposed weights of the encoder as the decoder.\n",
        "            # This is equivalent to doing: x = x @ (encoder_weights)^T\n",
        "            x = F.linear(x, self.fc_encoder.weight.t())\n",
        "        else:\n",
        "            # Use a separate decoder layer.\n",
        "            x = self.fc_decoder(x)\n",
        "\n",
        "        # Return a tuple:\n",
        "        # - x: the reconstructed input.\n",
        "        # - x_logit: the output of the classifier (if eval_classifier is True, else None).\n",
        "        return x, x_logit\n",
        "\n",
        "# Instantiate the model\n",
        "mtae = MTAutoEncoder()\n",
        "\n",
        "# Print the model architecture (if you run this cell, you'll see the details of the layers)\n",
        "mtae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK8W0nMSje7O"
      },
      "source": [
        "## Defining Autoencoder class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1pd8JMdje7O",
        "outputId": "b7de16d9-3631-4d1e-cf32-84d3a3f7730c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MTAutoEncoder(\n",
              "  (fc_encoder): Linear(in_features=990, out_features=200, bias=True)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=200, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "class MTAutoEncoder(nn.Module):\n",
        "    def __init__(self, num_inputs=990,\n",
        "                 num_latent=200, tied=True,\n",
        "                 num_classes=2, use_dropout=False):\n",
        "        super(MTAutoEncoder, self).__init__()\n",
        "        # Save key parameters for later use in the model.\n",
        "        self.tied = tied               # Controls whether the decoder's weights are \"tied\" to the encoder's.\n",
        "        self.num_latent = num_latent   # Dimension of the latent (compressed) representation.\n",
        "\n",
        "        # --- ENCODER DEFINITION ---\n",
        "        # Define a linear layer that maps the input (of size 990) to the latent space (of size 200).\n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "\n",
        "        # --- DECODER DEFINITION ---\n",
        "        # If the weights are not tied, then define a separate linear layer for the decoder.\n",
        "        if not tied:\n",
        "            self.fc_decoder = nn.Linear(num_latent, num_inputs)\n",
        "\n",
        "        # Note: The encoder layer is redefined immediately here, which overwrites the previous definition.\n",
        "        # This redundancy may be unintentional, but the result is that the model uses this new definition.\n",
        "        self.fc_encoder = nn.Linear(num_inputs, num_latent)\n",
        "\n",
        "        # --- CLASSIFIER DEFINITION ---\n",
        "        # The classifier takes the latent representation and maps it to a single output.\n",
        "        # It can optionally use dropout to help prevent overfitting.\n",
        "        if use_dropout:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Dropout(p=0.5),            # Randomly drops 50% of units during training.\n",
        "                nn.Linear(self.num_latent, 1)  # Linear layer that maps latent space to one output.\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(self.num_latent, 1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, eval_classifier=False):\n",
        "        # --- ENCODING ---\n",
        "        # Pass the input 'x' through the encoder layer.\n",
        "        x = self.fc_encoder(x)\n",
        "        # Apply the tanh activation function to introduce non-linearity.\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # --- CLASSIFICATION (OPTIONAL) ---\n",
        "        # If we are in a mode to evaluate classification, pass the latent features through the classifier.\n",
        "        if eval_classifier:\n",
        "            x_logit = self.classifier(x)\n",
        "        else:\n",
        "            x_logit = None\n",
        "\n",
        "        # --- DECODING / RECONSTRUCTION ---\n",
        "        if self.tied:\n",
        "            # When using tied weights, the decoder uses the transpose of the encoder's weight matrix.\n",
        "            # This essentially reverses the encoding operation.\n",
        "            x = F.linear(x, self.fc_encoder.weight.t())\n",
        "        else:\n",
        "            # Use the separate decoder layer if weights are not tied.\n",
        "            x = self.fc_decoder(x)\n",
        "\n",
        "        # The function returns a tuple:\n",
        "        # - x: the reconstructed input.\n",
        "        # - x_logit: the classification output (if computed) or None.\n",
        "        return x, x_logit\n",
        "\n",
        "# Create an instance of the autoencoder.\n",
        "mtae = MTAutoEncoder()\n",
        "\n",
        "# Display the model's architecture.\n",
        "mtae"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLxPmE4kje7O"
      },
      "source": [
        "## Defining training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Glfo2XVQje7O"
      },
      "outputs": [],
      "source": [
        "def train(model, epoch, train_loader, p_bernoulli=None, mode='both', lam_factor=1.0):\n",
        "    model.train()  # Set the model to training mode.\n",
        "    train_losses = []  # To store losses for each batch.\n",
        "\n",
        "    # Loop over each batch from the train_loader.\n",
        "    for i, (batch_x, batch_y) in enumerate(train_loader):\n",
        "        # Skip batches that are not full-sized.\n",
        "        if len(batch_x) != batch_size:\n",
        "            continue\n",
        "\n",
        "        # If p_bernoulli is provided, create a random binary mask (noise) for data corruption.\n",
        "        if p_bernoulli is not None:\n",
        "            # Only need to create this mask once, when i == 0.\n",
        "            if i == 0:\n",
        "                p_tensor = torch.ones_like(batch_x).to(device) * p_bernoulli\n",
        "            # Generate random 0/1 values based on the probability p_bernoulli.\n",
        "            rand_bernoulli = torch.bernoulli(p_tensor).to(device)\n",
        "\n",
        "        # Move the input data and labels to the device (CPU or GPU).\n",
        "        data, target = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "        # Reset gradients before the backward pass.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # *** Autoencoder (AE) Reconstruction Loss ***\n",
        "        # This branch handles the autoencoder part (reconstructing the input data).\n",
        "        if mode in ['both', 'ae']:\n",
        "            if p_bernoulli is not None:\n",
        "                # Multiply data by the random binary mask to add noise.\n",
        "                rec_noisy, _ = model(data * rand_bernoulli, False)\n",
        "                # Compute the reconstruction loss comparing the noisy reconstruction with the clean data.\n",
        "                loss_ae = criterion_ae(rec_noisy, data) / len(batch_x)\n",
        "            else:\n",
        "                # If no noise is added, just reconstruct the original data.\n",
        "                rec, _ = model(data, False)\n",
        "                loss_ae = criterion_ae(rec, data) / len(batch_x)\n",
        "\n",
        "        # *** Classifier Loss ***\n",
        "        # This branch handles the classifier part (predicting the labels).\n",
        "        if mode in ['both', 'clf']:\n",
        "            # Get the reconstruction and the classifier logits from the model.\n",
        "            rec_clean, logits = model(data, True)\n",
        "            # Compute the classification loss using the predicted logits and the true labels.\n",
        "            loss_clf = criterion_clf(logits, target)\n",
        "\n",
        "        # Combine the losses based on the chosen mode.\n",
        "        if mode == 'both':\n",
        "            # When both tasks are being trained, add the AE loss and the classifier loss,\n",
        "            # with the classifier loss scaled by lam_factor.\n",
        "            loss_total = loss_ae + lam_factor * loss_clf\n",
        "            # Save the losses for later analysis.\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(),\n",
        "                                 loss_clf.detach().cpu().numpy()])\n",
        "        elif mode == 'ae':\n",
        "            loss_total = loss_ae\n",
        "            train_losses.append([loss_ae.detach().cpu().numpy(), 0.0])\n",
        "        elif mode == 'clf':\n",
        "            loss_total = loss_clf\n",
        "            train_losses.append([0.0, loss_clf.detach().cpu().numpy()])\n",
        "\n",
        "        # *** Backward Pass and Optimization ***\n",
        "        loss_total.backward()  # Compute gradients.\n",
        "        optimizer.step()       # Update the model's parameters.\n",
        "\n",
        "    # Return a list of the losses for each batch in this epoch.\n",
        "    return train_losses\n",
        "\n",
        "def test(model, criterion, test_loader,\n",
        "         eval_classifier=False, num_batch=None):\n",
        "    test_loss, n_test, correct = 0.0, 0, 0\n",
        "    all_predss = []  # To store all predictions for classification.\n",
        "\n",
        "    # If evaluating the classifier, we'll keep track of true and predicted values.\n",
        "    if eval_classifier:\n",
        "        y_true, y_pred = [], []\n",
        "\n",
        "    # No gradient computation during testing.\n",
        "    with torch.no_grad():\n",
        "        model.eval()  # Set the model to evaluation mode.\n",
        "\n",
        "        # Loop over the test data batches.\n",
        "        for i, (batch_x, batch_y) in enumerate(test_loader, 1):\n",
        "            # If a specific number of batches is desired, limit the loop.\n",
        "            if num_batch is not None:\n",
        "                if i >= num_batch:\n",
        "                    continue\n",
        "\n",
        "            # Move the data to the appropriate device.\n",
        "            data = batch_x.to(device)\n",
        "            # Get the reconstructed data and classifier logits from the model.\n",
        "            rec, logits = model(data, eval_classifier)\n",
        "\n",
        "            # Compute the reconstruction loss and add it to the total loss.\n",
        "            test_loss += criterion(rec, data).detach().cpu().numpy()\n",
        "            n_test += len(batch_x)\n",
        "\n",
        "            # If classifier evaluation is turned on, process classification outputs.\n",
        "            if eval_classifier:\n",
        "                # Apply a sigmoid to convert logits to probabilities.\n",
        "                proba = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "                # Initialize predictions as ones (class 1).\n",
        "                preds = np.ones_like(proba, dtype=np.int32)\n",
        "                # Assign class 0 for probabilities below 0.5.\n",
        "                preds[proba < 0.5] = 0\n",
        "                all_predss.extend(preds)  # Collect predictions.\n",
        "\n",
        "                # Convert true labels to a NumPy array.\n",
        "                y_arr = np.array(batch_y, dtype=np.int32)\n",
        "\n",
        "                # Count correct predictions.\n",
        "                correct += np.sum(preds == y_arr)\n",
        "                y_true.extend(y_arr.tolist())\n",
        "                y_pred.extend(proba.tolist())\n",
        "\n",
        "        # Compute additional metrics using a custom confusion function.\n",
        "        mlp_acc, mlp_sens, mlp_spef = confusion(y_true, all_predss)\n",
        "\n",
        "    # Return the computed accuracy, sensitivity, and specificity.\n",
        "    return mlp_acc, mlp_sens, mlp_spef  # ,correct/n_test (this part is commented out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLBvWm1Zje7O",
        "outputId": "42632d8c-2722-4c92-a428-a91b4351409a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "scrolled": true,
        "id": "bjxpvdXUje7P"
      },
      "outputs": [],
      "source": [
        "if p_Method == \"ASD-DiagNet\" and p_mode == \"whole\":\n",
        "\n",
        "    # Get the number of correlations (features) from the first sample.\n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "    print(\"num_corr:  \", num_corr)\n",
        "\n",
        "    start = time.time()  # Record the start time for overall runtime.\n",
        "\n",
        "    # Set training hyperparameters.\n",
        "    batch_size = 8\n",
        "    learning_rate_ae, learning_rate_clf = 0.0001, 0.0001  # Learning rates for the autoencoder and classifier.\n",
        "    num_epochs = 25  # Total number of training epochs.\n",
        "\n",
        "    p_bernoulli = None  # No noise injection in this configuration.\n",
        "    augmentation = p_augmentation  # Flag for data augmentation.\n",
        "    use_dropout = False  # Whether to use dropout in the classifier.\n",
        "\n",
        "    # Augmentation and similarity parameters.\n",
        "    aug_factor = 2\n",
        "    num_neighbs = 5\n",
        "    lim4sim = 2\n",
        "    n_lat = int(num_corr / 4)  # Initial latent space size based on a fraction of the total correlations.\n",
        "    print(n_lat)\n",
        "    start = time.time()  # (Re)start the timer for training.\n",
        "\n",
        "    # Print the current configuration.\n",
        "    print('p_bernoulli: ', p_bernoulli)\n",
        "    print('augmentaiton: ', augmentation, 'aug_factor: ', aug_factor,\n",
        "          'num_neighbs: ', num_neighbs, 'lim4sim: ', lim4sim)\n",
        "    print('use_dropout: ', use_dropout, '\\n')\n",
        "\n",
        "    # Create a similarity function with a fixed limit using functools.partial.\n",
        "    sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "\n",
        "    crossval_res_kol = []  # List to store cross-validation results.\n",
        "\n",
        "    # Get labels for all files using a helper function.\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "    flist = np.array(flist)  # Convert the file list to a NumPy array.\n",
        "    kk = 0  # Initialize a counter (though it will be redefined in the loop below).\n",
        "\n",
        "    # Repeat the entire cross-validation process 10 times.\n",
        "    for rp in range(10):\n",
        "        # Set up stratified k-fold cross-validation.\n",
        "        kf = StratifiedKFold(n_splits=p_fold, random_state=1, shuffle=True)\n",
        "\n",
        "        # Shuffle the list of files to randomize the splits.\n",
        "        np.random.shuffle(flist)\n",
        "\n",
        "        # Update labels after shuffling.\n",
        "        y_arr = np.array([get_label(f) for f in flist])\n",
        "\n",
        "        # Loop through each fold.\n",
        "        for kk, (train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "            # Select training and testing samples based on indices.\n",
        "            train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "            # Set verbose to True for the first fold to see detailed output.\n",
        "            verbose = (True if (kk == 0) else False)\n",
        "\n",
        "            # Select regions (features) from the training samples.\n",
        "            regions_inds = get_regs(train_samples, int(num_corr / 4))\n",
        "\n",
        "            # Determine the number of inputs for the model based on the selected regions.\n",
        "            num_inpp = len(regions_inds)\n",
        "            # Set the latent dimension to half of the number of inputs.\n",
        "            n_lat = int(num_inpp / 2)\n",
        "\n",
        "            # Create a DataLoader for training data.\n",
        "            train_loader = get_loader(\n",
        "                data=all_corr,\n",
        "                samples_list=train_samples,\n",
        "                batch_size=batch_size,\n",
        "                mode='train',\n",
        "                augmentation=augmentation,\n",
        "                aug_factor=aug_factor,\n",
        "                num_neighbs=num_neighbs,\n",
        "                eig_data=eig_data,\n",
        "                similarity_fn=sim_function,\n",
        "                verbose=verbose,\n",
        "                regions=regions_inds\n",
        "            )\n",
        "\n",
        "            # Create a DataLoader for testing data.\n",
        "            test_loader = get_loader(\n",
        "                data=all_corr,\n",
        "                samples_list=test_samples,\n",
        "                batch_size=batch_size,\n",
        "                mode='test',\n",
        "                augmentation=False,\n",
        "                verbose=verbose,\n",
        "                regions=regions_inds\n",
        "            )\n",
        "\n",
        "            # Initialize the model with tied weights.\n",
        "            model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "            model.to(device)  # Move the model to GPU/CPU.\n",
        "\n",
        "            # Define the loss functions:\n",
        "            # Autoencoder reconstruction loss.\n",
        "            criterion_ae = nn.MSELoss(reduction='sum')\n",
        "            # Classifier loss (binary cross-entropy with logits).\n",
        "            criterion_clf = nn.BCEWithLogitsLoss()\n",
        "\n",
        "            # Set up the optimizer to update the encoder and classifier parameters.\n",
        "            optimizer = optim.SGD(\n",
        "                [{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                 {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                momentum=0.9\n",
        "            )\n",
        "\n",
        "            # Train the model over multiple epochs.\n",
        "            for epoch in range(1, num_epochs + 1):\n",
        "                # For the first 20 epochs, train both autoencoder and classifier.\n",
        "                if epoch <= 20:\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                else:\n",
        "                    # After 20 epochs, train only the classifier part.\n",
        "                    train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "            # Evaluate the model on the test set.\n",
        "            res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
        "            # Print test results for this fold.\n",
        "            print(test(model, criterion_ae, test_loader, eval_classifier=True))\n",
        "\n",
        "            # Save the result for this fold.\n",
        "            crossval_res_kol.append(res_mlp)\n",
        "\n",
        "        # After finishing all folds, print the average results for this repeat.\n",
        "        print(\"averages:\")\n",
        "        print(np.mean(np.array(crossval_res_kol), axis=0))\n",
        "        finish = time.time()\n",
        "        # Print the running time for this repeat.\n",
        "        print(finish - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Per Site p_mode = \"percenter\" and p_center = \" \""
      ],
      "metadata": {
        "id": "pcoMQiTkUjho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p_ROI = \"cc200\"\n",
        "p_center = \"CMU\"  # Change to the desired center\n",
        "p_mode = \"percenter\" #Change to percenter\n",
        "p_Method = \"ASD-DiagNet\"\n",
        "p_fold = 5  # Desired number of folds for larger centers\n",
        "p_augmentation = True"
      ],
      "metadata": {
        "id": "Frrs64AvAfqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "1oKImd8Eje7P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "7104029d-4ce8-497f-c9a9-8511ba01ca0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 2-fold cross-validation for center CMU.\n",
            "Running repeat 1 for center CMU...\n",
            "Computing neighbors for augmentation...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "cal_similarity() missing 1 required positional argument: 'weights'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-fb4314e029b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mn_lat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_inpp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 train_loader = get_loader(data=all_corr, samples_list=train_samples, \n\u001b[0m\u001b[1;32m     59\u001b[0m                                           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                                           \u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-aa8d37777216>\u001b[0m in \u001b[0;36mget_loader\u001b[0;34m(data, samples_list, batch_size, mode, augmentation, aug_factor, num_neighbs, eig_data, similarity_fn, verbose, regions)\u001b[0m\n\u001b[1;32m    112\u001b[0m                        \u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                        eig_data=None, similarity_fn=None, verbose=False, regions=None):\n\u001b[0;32m--> 114\u001b[0;31m             dataset = ASDDataset(data, samples_list, mode, augmentation, \n\u001b[0m\u001b[1;32m    115\u001b[0m                                 \u001b[0maug_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neighbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meig_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                                 similarity_fn, verbose, regions)\n",
            "\u001b[0;32m<ipython-input-20-aa8d37777216>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, samples_list, mode, augmentation, aug_factor, num_neighbs, eig_data, similarity_fn, verbose, regions)\u001b[0m\n\u001b[1;32m     66\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mf2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                                 \u001b[0msim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarity_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meig_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meig_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0msim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only include actually similar samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                     \u001b[0msimilar_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cal_similarity() missing 1 required positional argument: 'weights'"
          ]
        }
      ],
      "source": [
        "if p_Method == \"ASD-DiagNet\" and p_mode == \"percenter\":\n",
        "    # Determine the number of correlations (features) available from the first file.\n",
        "    num_corr = len(all_corr[flist[0]][0])\n",
        "\n",
        "    # List all files in the main data path and process them to extract a key.\n",
        "    flist = os.listdir(data_main_path)\n",
        "    flist = [get_key(f) for f in flist]\n",
        "\n",
        "    # Group files by centers (e.g., different imaging centers)\n",
        "    centers_dict = {}\n",
        "    for f in flist:\n",
        "        key = f.split('_')[0]  # Assume the center ID is the first part of the filename.\n",
        "        centers_dict.setdefault(key, []).append(f)\n",
        "\n",
        "    # Select files that belong to the current center (p_center).\n",
        "    flist = np.array(centers_dict[p_center])\n",
        "\n",
        "    # Get labels for each file (e.g., indicating ASD or not) using a helper function.\n",
        "    y_arr = np.array([get_label(f) for f in flist])\n",
        "\n",
        "    # Determine the number of splits for cross-validation based on class balance.\n",
        "    unique_labels, counts = np.unique(y_arr, return_counts=True)\n",
        "    new_n_splits = min(p_fold, counts.min())  # Ensure every fold has at least one sample of each class.\n",
        "\n",
        "    if new_n_splits < 2:\n",
        "        print(f\"Skipping center {p_center} due to insufficient samples in one class.\")\n",
        "    else:\n",
        "        print(f\"Using {new_n_splits}-fold cross-validation for center {p_center}.\")\n",
        "\n",
        "        start = time.time()  # Start timing the process.\n",
        "\n",
        "        # Set training parameters.\n",
        "        batch_size = 8\n",
        "        learning_rate_ae, learning_rate_clf = 0.0001, 0.0001\n",
        "        num_epochs = 25\n",
        "        p_bernoulli = None  # This can be used for noise injection.\n",
        "        augmentation = p_augmentation  # Whether to perform data augmentation.\n",
        "        use_dropout = False\n",
        "        aug_factor = 2\n",
        "        num_neighbs = 5\n",
        "        lim4sim = 2\n",
        "        n_lat = int(num_corr / 4)  # Initial latent dimension (will be updated later).\n",
        "\n",
        "        # Create a partial function for calculating similarity with a fixed limit.\n",
        "        sim_function = functools.partial(cal_similarity, lim=lim4sim)\n",
        "        all_rp_res = []  # To store results for each repeat.\n",
        "\n",
        "        # Repeat the entire cross-validation process 10 times for stability.\n",
        "        for rp in range(10):\n",
        "            print(f\"Running repeat {rp + 1} for center {p_center}...\")\n",
        "            crossval_res_kol = []  # Store results for each fold in this repeat.\n",
        "\n",
        "            # Set up stratified k-fold cross-validation.\n",
        "            kf = StratifiedKFold(n_splits=new_n_splits)\n",
        "            for kk, (train_index, test_index) in enumerate(kf.split(flist, y_arr)):\n",
        "                # Get training and testing samples based on indices.\n",
        "                train_samples, test_samples = flist[train_index], flist[test_index]\n",
        "\n",
        "                # For the first fold, set verbose to True to get extra output.\n",
        "                verbose = (kk == 0)\n",
        "\n",
        "                # Determine which regions (features) to use; here, it's a subset of the correlations.\n",
        "                regions_inds = get_regs(train_samples, int(num_corr / 4))\n",
        "                num_inpp = len(regions_inds)  # Number of input features for the model.\n",
        "                n_lat = int(num_inpp / 2)      # Set the latent space size to half the number of inputs.\n",
        "\n",
        "                # Create DataLoaders for training and testing using a helper function.\n",
        "                train_loader = get_loader(\n",
        "                    data=all_corr,\n",
        "                    samples_list=train_samples,\n",
        "                    batch_size=batch_size,\n",
        "                    mode='train',\n",
        "                    augmentation=augmentation,\n",
        "                    aug_factor=aug_factor,\n",
        "                    num_neighbs=num_neighbs,\n",
        "                    eig_data=eig_data,\n",
        "                    similarity_fn=sim_function,\n",
        "                    verbose=verbose,\n",
        "                    regions=regions_inds\n",
        "                )\n",
        "\n",
        "                test_loader = get_loader(\n",
        "                    data=all_corr,\n",
        "                    samples_list=test_samples,\n",
        "                    batch_size=batch_size,\n",
        "                    mode='test',\n",
        "                    augmentation=False,\n",
        "                    verbose=verbose,\n",
        "                    regions=regions_inds\n",
        "                )\n",
        "\n",
        "                # Initialize the model with tied weights, using the determined number of input features and latent dimension.\n",
        "                model = MTAutoEncoder(tied=True, num_inputs=num_inpp, num_latent=n_lat, use_dropout=use_dropout)\n",
        "                model.to(device)  # Move the model to GPU/CPU.\n",
        "\n",
        "                # Define loss functions for autoencoder (reconstruction) and classifier.\n",
        "                criterion_ae = nn.MSELoss(reduction='sum')\n",
        "                criterion_clf = nn.BCEWithLogitsLoss()\n",
        "\n",
        "                # Set up the optimizer to update encoder and classifier parameters with separate learning rates.\n",
        "                optimizer = optim.SGD(\n",
        "                    [{'params': model.fc_encoder.parameters(), 'lr': learning_rate_ae},\n",
        "                     {'params': model.classifier.parameters(), 'lr': learning_rate_clf}],\n",
        "                    momentum=0.9\n",
        "                )\n",
        "\n",
        "                # Training loop for the specified number of epochs.\n",
        "                for epoch in range(1, num_epochs + 1):\n",
        "                    if epoch <= 20:\n",
        "                        # In early epochs, train both the autoencoder (AE) and classifier.\n",
        "                        train_losses = train(model, epoch, train_loader, p_bernoulli, mode='both')\n",
        "                    else:\n",
        "                        # In later epochs, focus only on training the classifier.\n",
        "                        train_losses = train(model, epoch, train_loader, p_bernoulli, mode='clf')\n",
        "\n",
        "                # Evaluate the model on the test set.\n",
        "                res_mlp = test(model, criterion_ae, test_loader, eval_classifier=True)\n",
        "                crossval_res_kol.append(res_mlp)\n",
        "\n",
        "            # Print the average result for this repeat (across all folds).\n",
        "            print(f\"Result of repeat {rp + 1} for center {p_center}: {np.mean(np.array(crossval_res_kol), axis=0)}\")\n",
        "            all_rp_res.append(np.mean(np.array(crossval_res_kol), axis=0))\n",
        "\n",
        "        # Print the overall average result across all repeats.\n",
        "        print(f\"Average result for 10 repeats for center {p_center}: {np.mean(np.array(all_rp_res), axis=0)}\")\n",
        "        finish = time.time()\n",
        "        print(f\"Total running time for center {p_center}: {finish - start:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result Paper - Result Mine:\n",
        "\n",
        "Accuracy%\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Caltech: 52.8% - 62.75%\n",
        "\n",
        "CMU: 68.5% - ERROR\n",
        "\n",
        "KKI: 69.5% - 69.3%\n",
        "\n",
        "Leuven: 61.3% - 54.1%\n",
        "\n",
        "Maxmun: 48.6% - 63.9%\n",
        "\n",
        "NYU: 68.0% - 67.9%\n",
        "\n",
        "OHSU: 82% - 73.3%\n",
        "\n",
        "Olin: 65.1% - 68.8%\n",
        "\n",
        "Pitt: 67.8% - 63.3%\n",
        "\n",
        "SBL: 51.6% - 43.6%\n",
        "\n",
        "SDSU: 63.0% - 64.0%\n",
        "\n",
        "Stanford: 64.2% - 64.9%\n",
        "\n",
        "Trinity: 54.1% - 53.1%\n",
        "\n",
        "UCLA: 73.2% - 67.1%\n",
        "\n",
        "USM: 68.2% - 68.2%\n",
        "\n",
        "UM: 63.8% - 65.7%\n",
        "\n",
        "Yale: 63.6% - 64.1%\n"
      ],
      "metadata": {
        "id": "mr6tGxUaCVMd"
      }
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
